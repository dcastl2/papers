\section{Models of Memory}

Ebbinghaus is credited with a theory of memory and forgetting which has
withstood empirical study for over a century \cite{ebbinghaus}. It is known as
the power law of forgetting.  According to the power law of forgetting, the
strength of a memory after a time $t$ falls off exponentially: 

\begin{equations}
\label{eq:ebbinghaus}
 S(t) = ae^{-bt}
\end{equations}

In this model, $a$ is the initial strength of the memory, and $b^{-1}$ is a
decay rate.  The curve drawn by this function is known as the curve of
forgetting, which is depicted in Figure~\ref{fig:forgetting}.  If $a=1$, the
function may be interpreted as a probability function:

\begin{equations}
\label{eq:ebbinghaus-p}
 p_{recall}(t) = e^{-bt}
\end{equations}

\begin{figure}[p!]
 \label{fig:forgetting}
 \includegraphics{fig/forgetting.eps} 
 \caption{The Ebbinghaus curve of forgetting, which features an exponential
 dropoff of memory strength over time}
\end{figure}

\begin{figure}[p!]
 \label{fig:modified}
 \includegraphics{fig/modified.eps} 
 \caption{The modified curve of forgetting, which resembles the reverse
 sigmoid function}
\end{figure}

According to the theory, if the memory strength falls below a certain
threshold, then in the absence of any intervening information (that is,
information which re-activates the memory via association), the individual will
be unable to spontaneously recall the information.  In a probabilistic model,
one may set the threshold at .5 probability---the probability below which the
individual has more than likely forgotten the information. 

\section{ACT-R}

As has been mentioned in Sec~\ref{sec:litreview}, John Anderson is credited
with having developed Adaptive Control of Thought-Rational (ACT-R), a
process-based model which simulates the solving of problems.  In ACT-R, there
are goals, akin to problem statements; and rules, or processes used to solve
problems; and finally facts, or knowledge utilized in the course of applying
rules. 

In addition to this, however, Anderson added models for memory and forgetting
to support realistic recall probabilities and latencies.  The memory component
is based on Ebbinghaus' model of memory retrieval.  Anderson added a component
to explain memory re-activation of a memory.  According to Anderson's model, a
chunk of memory $i$ is re-activated (or additionally activated) to the extent
that other chunks of information (related concepts, words, ideas, etc.) which
have some association to $i$ are attended to.  This notion is captured in the
following equation \cite{anderson2000implications}:

\begin{equations}
\label{eq:anderson-activation}
a_i = b_i + \displaystyle\sum_{i=j}^n w_j s_{ji}
\end{equations}

In this equation, known as the activation equation, the activation of a chunk
$i$ is equal to its base activation $b_i$, plus the products of the attentional
weights $w_j$ by the associative strength of $s_{ji}$ to other chunks.  This
provides an intuitive explanation for the manner in which recall of a target
chunk can be stimulated by dropping hints, using certain key words or phrases,
or mentioning related material. 

Practice has the effect of causing the base strength of the memory to increase,
and delays cause the strength of the memory to drop off
\cite{anderson2000implications}:  

\begin{equations}
\label{eq:anderson-spacing}
b_i = \mathrm{ln} \Bigg( \displaystyle\sum_{j=1}^n t_j^{-d} \Bigg)
\end{equations}

Here, $t_j$ is the time since the jth practice of an item, and $d$ is a decay
rate. 

Some concepts, particularly the notion of re-activation of memories, have been
borrowed from ACT-R and modified to fit the more coarse-grained intelligent
tutoring system presented in this work.  In particular, total problems rather
than individial processes will have probabilities of recall associated with
them.  Also, associative strengths are established using a factor analysis.

\begin{figure}[p!]
 \label{fig:memory}
 %TODO: (SRB) the x-axis of this graph is messed up.
 \includegraphics{fig/memory.eps} 
 \caption{Forgetting with re-activation; each spike in the graph is an
 additional trial where the student is exposed to the item again}
\end{figure}

\begin{figure}[p!]
 \label{fig:spacing}
 \includegraphics{fig/spacing.eps} 
 \caption{The learning curve, which indicates the extent of memory
 lifespan increase given a time between trials}
\end{figure}

\section{Alterations to Memory Model}

A slight modification to this theory accounts for short-term memory and
short-term memorization, which allows for a small time window for the student
to enjoy a high probability of recollection before dropping off sharply, as in
the original curve:

\begin{equations}
\label{eq:memory}
 p_{recall}(t) = \frac{1}{1 + e^{{m(t-\lambda)}}}
\end{equations}

In this equation, $\lambda$ is the lifespan of the memory; or, the amount of
time that passes until there remains only a .5 probability that the student
recalls the information.  The value $m$ is a parameter which controls the rate
of dropoff, much like the decay rate in Ebbinghaus' model.  An example curve
for this equation is given in Figure~\ref{fig:modified}.


\section{Re-Activation}
\label{sec:reactivation}

To account for re-activation, a simple model for the extension of half-life
may be used: 

\begin{equations}
\label{eq:lambda-rho}
 \lambda_n = \rho_s \lambda_{n-1}
\end{equations}

Here, $n$ refers to exposure or trial number $n$.  In the intelligent tutoring
system, this is the nth time that the student has seen the problem.
$\lambda_{n-1}$ is the former lifespan of the memory.  $\rho_s$ is a learning
rate, which is a parameter particular to the student; its domain is (1,
$\infty$].  The intuition captured by this formula is that with an increased
number of trials, the lifespan of the memory increases.

Apparently there is a difference in problems in the ease with which they are
learned.  An addendum to this can be used to account for individual differences
in problems: 

\begin{equations}
\label{eq:lambda-mu}
 \lambda_n = \mu_i \rho_s \lambda_{n-1}
\end{equations}

Here, $\mu_i$ represents the memorability of the problem, or the ease with
which the problem solution can be committed to memory. 

% TODO: relationship to power law of learning or learning curve.
% TODO: show relationship between Anderson and this?

\subsection{Spacing Effect}

The spacing effect is the effect that the amount of time in between trials has
on the memorization of a chunk of memory.  In the above model, memorization is
interpreted as an increase in the lifespan of a memory.  If only a short amount
of time passes between the last trial, the effect will not be as great as if a
longer time has passed.  One consequence of this is that, according to the
spacing effect hypothesis, cramming is ineffective (where cramming is namely
repeating trials in short bursts).

The spacing effect can be accommodated in the memory model used by the
intelligent tutoring system.  We define a function for the dropoff:

\begin{equations}
  \label{eq:spacing}
  \sigma_t = (1 - e^{-at})
\end{equations}

This indicates the extent to which the spacing from the time the item was last
seen influences the increase in the lifespan of the memory: 

\begin{equations}
\label{eq:lambda-final}
 \lambda_n = (1 + \sigma_t \mu_i \rho_s) \lambda_{n-1}
\end{equations}

The utility of this model is in assessing the probability with which a student
answers a question; not only based on trait ability and dependency
relationships, but also on the inherent tendency to forget information with the
passage of time.

It will be assumed that, if a student has been exposed to a item before, then
the probability of being able to answer the item correctly may assume one of
two values.  The first is based upon recollection; it is the probability of
recalling the facts, processes, and so forth required to produce the solution
for an item.  The second is based upon derivation of the solution from known
facts, processes, and so forth in the dependencies, as if the student were
answering the question for the first time.  That is, if a student does not
recall the process for solving a problem, the probability defaults to the
probability based upon item parameters, trait ability and dependency
relationships:

\begin{equations}
\label{eq:p-final}
p =\left\{
         \begin{array}{ll}
               p_{recall}(t) & \mathrm{if}\  p_{recall} > p({\theta_s, x_1, \ldots}) \\
               p({\theta_s, x_1, \ldots}) & \mathrm{otherwise}
         \end{array}
       \right.
\end{equations}

Now the intelligent tutoring system has models of dependency relationships
among questions and probability estimates for questions, and all other
mathematical equipment required to schedule problems. 



