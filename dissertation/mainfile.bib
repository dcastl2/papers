Supports some nifty features:
- estimation of concept relation degree
- personalized learning path based on genetic algorithm
@article{chen2008intelligent,
  title={Intelligent web-based learning system with personalized learning path guidance},
  author={Chen, Chih-Ming},
  journal={Computers \& Education},
  volume={51},
  number={2},
  pages={787--814},
  year={2008},
  publisher={Elsevier}
}


Four ways of developing test:
1. heuristic based test assembly: find one question at a time.
2. 0-1 linear programming
3. network flow programming
4. optimal design
@article{van1998optimal,
  title={Optimal assembly of psychological and educational tests},
  author={Van der Linden, Wim J},
  journal={Applied Psychological Measurement},
  volume={22},
  number={3},
  pages={195--211},
  year={1998},
  publisher={SAGE PUBLICATIONS, INC. 2455 Teller Road, Thousand Oaks, CA 91320}
}


Several criteria are presented for evaluating a CAT.
@article{davey2011guide,
  title={A Guide to Computer Adaptive Testing Systems.},
  author={Davey, Tim},
  journal={Council of Chief State School Officers},
  year={2011},
  publisher={ERIC}
}


Interestingly, has some testing algorithms. Covers ability estiamtes.
@book{wainer2000computerized,
  title={Computerized adaptive testing: A primer},
  author={Wainer, Howard and Dorans, Neil J and Flaugher, Ronald and Green, Bert F and Mislevy, Robert J},
  year={2000},
  publisher={Routledge}
}


Outlines the five processes in CAT:
1. initiation of ability estimate
2. selection of item
3. administration of item
4. updating ability estimate
5. check stopping criterion
@article{veldkamp2013bayesian,
  title={Bayesian computerized adaptive testing},
  author={Veldkamp, Bernard P and Matteucci, Mariagiulia},
  journal={Ensaio: Avalia{\c{c}}{\~a}o e Pol{\'\i}ticas P{\'u}blicas em Educa{\c{c}}{\~a}o},
  volume={21},
  number={78},
  pages={57--82},
  year={2013},
  publisher={SciELO Brasil}
}


Contains some information on ability estimation, shadow testing (tests designed
to select items), and multidimensional CAT (which is my model).
@book{van2000computerized,
  title={Computerized adaptive testing: Theory and practice},
  author={Van der Linden, Wim J and Glas, Cees AW and others},
  year={2000},
  publisher={Springer}
}



Feedback effects are large.
@article{bangert1991instructional,
  title={The instructional effect of feedback in test-like events},
  author={Bangert-Drowns, Robert L and Kulik, Chen-Lin C and Kulik, James A and Morgan, MaryTeresa},
  journal={Review of educational research},
  volume={61},
  number={2},
  pages={213--238},
  year={1991},
  publisher={Sage Publications Sage CA: Thousand Oaks, CA}
}


The effects of the medium of test administration, paper and pencil vs
computerized, were examined for timed power and speeded tests of cognitive
abilities for populations of young adults and adults. Meta-analytic techniques
were used to estimate the cross-mode correlation after correcting for
measurement error.
@article{mead1993equivalence,
  title={Equivalence of computerized and paper-and-pencil cognitive ability tests: A meta-analysis.},
  author={Mead, Alan D and Drasgow, Fritz},
  journal={Psychological Bulletin},
  volume={114},
  number={3},
  pages={449},
  year={1993},
  publisher={American Psychological Association}
}


Curriculum sequencing using PWIS: personalized web based instruction system.
"Courseware", or questions, are given to students; then recommendations are
made based on ability levels. 
@article{chen2006personalized,
  title={Personalized curriculum sequencing utilizing modified item response theory for web-based instruction},
  author={Chen, Chih-Ming and Liu, Chao-Yu and Chang, Mei-Hui},
  journal={Expert Systems with applications},
  volume={30},
  number={2},
  pages={378--396},
  year={2006},
  publisher={Elsevier}
}


For IRT/MLE
@book{hambleton1991fundamentals,
  title={Fundamentals of item response theory},
  author={Hambleton, Ronald K and Swaminathan, Hariharan and Rogers, H Jane},
  year={1991},
  publisher={Sage}
}


This study proposes a personalized e-learning system based on Item Response
Theory (PEL-IRT) which considers both course material difficulty and learner
ability to provide individual learning paths for learners. The item
characteristic function proposed by Rasch with a single difficulty parameter is
used to model the course materials.
@article{chen2005personalized,
  title={Personalized e-learning system using item response theory},
  author={Chen, Chih-Ming and Lee, Hahn-Ming and Chen, Ya-Hui},
  journal={Computers \& Education},
  volume={44},
  number={3},
  pages={237--255},
  year={2005},
  publisher={Elsevier}
}


How to do MLE
@book{baker2004item,
  title={Item response theory: Parameter estimation techniques},
  author={Baker, Frank B and Kim, Seock-Ho},
  year={2004},
  publisher={CRC Press}
}


@article{ccepni2006effects,
  title={The effects of computer-assisted material on students’ cognitive levels, misconceptions and attitudes towards science},
  author={{\c{C}}epni, Salih and Ta{\c{s}}, Erol and K{\"o}se, Sacit},
  journal={Computers \& Education},
  volume={46},
  number={2},
  pages={192--205},
  year={2006},
  publisher={Elsevier}
}


"The creation of a technology-enriched classroom environment appears to bave
had a minimal but positive effect on student acquisition of higher-order
thinking skills. Although the difference in scores was not significant for
every level of Bloom's Taxonomy, the scores were generally higher for analysis
and synthesis and significantly higher for evaluation."
@article{hopson2001using,
  title={Using a technology-enriched environment to improve higher-order thinking skills},
  author={Hopson, Michael H and Simms, Richard L and Knezek, Gerald A},
  journal={Journal of Research on Technology in education},
  volume={34},
  number={2},
  pages={109--119},
  year={2001},
  publisher={Taylor \& Francis}
}

Revised Bloom's taxonomy is a matrix, but not of trait ability estimates.
No scheduling algorithm? Says how to start, but not follow through.
@inproceedings{raykova2011adaptive,
  title={Adaptive test system based on revised Bloom's taxonomy},
  author={Raykova, Mariana and Kostadinova, Hristina and Totkov, George},
  booktitle={Proceedings of the 12th International Conference on Computer Systems and Technologies},
  pages={504--509},
  year={2011},
  organization={ACM}
}

Success in using Bloom's taxonomy with MCQs.  In particular,
Synthesis/Evaluation are targetted by asking for diagnoses
given patient information.  Anatomy/physiology questions.
@inproceedings{de2011computer,
  title={Computer-based testing-the ideal tool to assess on the different levels of Bloom's taxonomy},
  author={De Bruyn, E and Mostert, E and Van Schoor, A},
  booktitle={Interactive Collaborative Learning (ICL), 2011 14th International Conference on},
  pages={444--449},
  year={2011},
  organization={IEEE}
}

Argues for questions that touch upper levels of Bloom's taxonomy
@inproceedings{hernan2008testing,
  title={Testing-Based Automatic Grading: a proposal from Bloom's taxonomy},
  author={Hern{\'a}n-Losada, Isidoro and Pareja-Flores, Crist{\'o}bal and Vel{\'a}zquez-Iturbide, J {\'A}ngel},
  booktitle={Advanced Learning Technologies, 2008. ICALT'08. Eighth IEEE International Conference on},
  pages={847--849},
  year={2008},
  organization={IEEE}
}

Theories of multimedia learning
@article{mayer2002multimedia,
  title={Multimedia learning},
  author={Mayer, Richard E},
  journal={Psychology of learning and motivation},
  volume={41},
  pages={85--139},
  year={2002},
  publisher={Elsevier}
}

Limitations of MCQs, discussion of good feedback practice:
@article{nicol2007assessment,
  title={E-assessment by design: using multiple-choice tests to good effect},
  author={Nicol, David},
  journal={Journal of Further and higher Education},
  volume={31},
  number={1},
  pages={53--64},
  year={2007},
  publisher={Taylor \& Francis}
}

Establishes some terminology.
@book{bull2003blueprint,
  title={A blueprint for computer-assisted assessment},
  author={Bull, Joanna and McKenna, Colleen},
  year={2003},
  publisher={Routledge}
}

Paterson (2002) indicated that it is not feasible to test the higher-level
cognitive skills using CAA within mathematics. Bloom states that in the
majority of instances Synthesis and Evaluation promote divergent thinking and
answers cannot be determined in advance (Bloom et al., 1971). Heinrich and Wang
(2003) argue that objective testing is still not sophisticated enough to
examine complex content and thinking patterns. However, other research in
linguistics and computer programming concluded that the higher-level skills can
be assessed via CAA through innovative approaches (1998). In the study by Reid
(2002) a new language was devised and students were required to apply
linguistic techniques in order to answer MCQ. 
@article{sim2004implementation,
  title={Implementation of computer assisted assessment: lessons from the literature},
  author={Sim, Gavin and Holifield, Phil and Brown, Martin},
  journal={ALT-J},
  volume={12},
  number={3},
  pages={215--229},
  year={2004},
  publisher={Taylor \& Francis}
}

Concerned with the creation of objective questions at the higher Bloom levels.
Outlines a method for doing so.  Shows that objective questions at the higher
levels can be created.
@article{duke2001using,
  title={Using computer-aided assessment to test higher level learning outcomes},
  author={Duke-Williams, Emma and King, Terry},
  year={2001},
  publisher={{\copyright} Loughborough University}
}

Stresses the need for a competency model, namely one that uses Bloom's taxonomy of
educational objectives.  Suggests that information in a competency model can be
used to personalize assessment.
@article{sitthisak2007towards,
  title={Towards a competency model for adaptive assessment to support lifelong learning},
  author={Sitthisak, Onjira and Gilbert, Lester and Davis, Hugh C},
  journal={Service Oriented Approaches and Lifelong Competence Development Infrastructures},
  pages={200},
  year={2007},
  publisher={Citeseer}
}

In biology, it is agreed that there is a matter of correctness about the answers
to higher-order questions.
@article{lemons2013questions,
  title={Questions for Assessing Higher-Order Cognitive Skills: It's Not Just Bloom’s},
  author={Lemons, Paula P and Lemons, J Derrick},
  journal={CBE-Life Sciences Education},
  volume={12},
  number={1},
  pages={47--58},
  year={2013},
  publisher={Am Soc Cell Biol}
}

The CAT prototype introduced here includes a proficiency level estimation
based on Item Response Theory and a questions’ database. The questions in the
database are classified according to topic area and difficulty level. The level
of difficulty estimate comprises expert evaluation based upon Bloom’s taxonomy
and users’ performance over time.
Apparently the difficulty is set to the Bloom level. No scheduler.
@article{lilley2005generation,
  title={The generation of automated learner feedback based on individual proficiency levels},
  author={Lilley, Mariana and Barker, Trevor and Britton, Carol},
  journal={Innovations in Applied Artificial Intelligence},
  pages={14--15},
  year={2005},
  publisher={Springer}
}

Mentioned IRT and bloom, but it is not clear how these are combined or how
they are scheduled. It seems that the assessments themselves are scheduled. 
This is a prototype system.
@article{yarandi2011personalised,
  title={Personalised mobile learning system based on item response theory},
  author={Yarandi, Maryam and Jahankhani, Hamid and Dastbaz, Mohammad and Tawil, Abdel-Rahman},
  year={2011}
}

examines several multiple choice formats 
@article{haladyna1992effectiveness,
  title={The effectiveness of several multiple-choice formats},
  author={Haladyna, Thomas M},
  journal={Applied Measurement in Education},
  volume={5},
  number={1},
  pages={73--88},
  year={1992},
  publisher={Taylor \& Francis}
}

empirical support for formative assessment, feedback-based learning
@article{lawton2012online,
  title={Online learning based on essential concepts and formative assessment},
  author={Lawton, Daryl and Vye, Nancy and Bransford, John and Sanders, Elizabeth and Richey, Michael and French, David and Stephens, Rick},
  journal={Journal of Engineering Education},
  volume={101},
  number={2},
  pages={244--287},
  year={2012},
  publisher={Wiley Online Library}
}

------------------------------------------------------------------

Overlaps with some parts of my research; although it states outright that Bloom=difficulty
@article{newman1988effect,
  title={Effect of varying item order on multiple-choice test scores: Importance of statistical and cognitive difficulty},
  author={Newman, Dianna L and Kundert, Deborah K and Lane Jr, David S and Bull, Kay Sather},
  journal={Applied Measurement in education},
  volume={1},
  number={1},
  pages={89--97},
  year={1988},
  publisher={Taylor \& Francis}
}


Very clear support for the thought that Bloom=difficulty, even used for course difficulty rating
@inproceedings{oliver2004course,
  title={This course has a Bloom Rating of 3.9},
  author={Oliver, Dave and Dobele, Tony and Greber, Myles and Roberts, Tim},
  booktitle={Proceedings of the Sixth Australasian Conference on Computing Education-Volume 30},
  pages={227--231},
  year={2004},
  organization={Australian Computer Society, Inc.}
}


Very clear support for the thought that Bloom=difficulty
@article{lord2007moving,
  title={Moving students from information recitation to information understanding: exploiting Bloom's taxonomy in creating science questions},
  author={Lord, Thomas and Baviskar, Sandhya},
  journal={Journal of College Science Teaching},
  volume={36},
  number={5},
  pages={40},
  year={2007},
  publisher={National Science Teachers Association}
}


Support for people thinking that Bloom=difficulty
@inproceedings{johnson2006bloom,
  title={Is Bloom's taxonomy appropriate for computer science?},
  author={Johnson, Colin G and Fuller, Ursula},
  booktitle={Proceedings of the 6th Baltic Sea conference on Computing education research: Koli Calling 2006},
  pages={120--123},
  year={2006},
  organization={ACM}
}


Support for people thinking that Bloom=difficulty
@inproceedings{fuller2007developing,
  title={Developing a computer science-specific learning taxonomy},
  author={Fuller, Ursula and Johnson, Colin G and Ahoniemi, Tuukka and Cukierman, Diana and Hern{\'a}n-Losada, Isidoro and Jackova, Jana and Lahtinen, Essi and Lewis, Tracy L and Thompson, Donna McGee and Riedesel, Charles et al.},
  booktitle={ACM SIGCSE Bulletin},
  volume={39:4},
  pages={152--170},
  year={2007},
  organization={ACM}
}


Support for Bloom!=difficulty: facility versus complexity
@article{hill1981testing,
  title={Testing the simplex assumption underlying Bloom's Taxonomy},
  author={Hill, PW and McGaw, B},
  journal={American Educational Research Journal},
  volume={18},
  number={1},
  pages={93--101},
  year={1981},
  publisher={Sage Publications}
}


Support for Bloom!=difficulty
@inproceedings{thompson2008bloom,
  title={Bloom's taxonomy for CS assessment},
  author={Thompson, Errol and Luxton-Reilly, Andrew and Whalley, Jacqueline L and Hu, Minjie and Robbins, Phil},
  booktitle={Proceedings of the tenth conference on Australasian computing education-Volume 78},
  pages={155--161},
  year={2008},
  organization={Australian Computer Society, Inc.}
}


Using item response theory (IRT) in computer adaptive test (CAT) is critical to
assess learners' readiness for further learning. This theory has assumptions
concerning the mathematical relationship between learner's abilities and item
responses. A numerical value from the theory has potential to decide who the
best learner is. The IRT score only provides the learner's result and lack of
the assessment of learner's ability level in cognitive skills. This paper
proposes an approach of using Bloom's cognitive skills for evaluating learned
ability level by considering achieved the level of difficulty and the IRT
score. Our starting point in the adaptive test is at a question of learners'
knowledge level difficulty that we got from their pre-test scores instead of
starting with a question of a medium difficulty. This approach provides the
effectiveness of CAT to test cognitive skills and offers some theoretical
considerations on linking learning outcomes and assessments.
@article{sitthisak,
  title={Cognitive Assessment Applying with Item Response Theory},
  author={Sitthisak, Onjira and Soonklang, Tasanawan and Gilbert, Lester},
  journal={19th Annual International Conference on Computers in Education},
  year={2011}
}


Bloom original
@book{bloom1956,
  title={Taxonomy of educational objectives},
  author={Bloom, Benjamin Samuel et al.},
  year={1956},
  publisher={David McKay}
}


Newton Raphson
@book{baker2004,
  title={Item response theory: Parameter estimation techniques},
  author={Baker, Frank B and Kim, Seock-Ho},
  year={2004},
  publisher={CRC Press}
}


R can be hooked to paper manuscripts to re-generate scientific findings.
@inproceedings{castleberry2011,
          author    = {Dennis Castleberry}
         ,title     = {The Prickly Pear Archive}
         ,journal   = {ICCS}
         ,booktitle = {International Conference on Computational Science (ICCS). Proceedings}
         ,address   = {Singapore, Singapore}
         ,publisher = {Elsevier}
         ,year      = {2011}
}

R can be hooked to paper manuscripts to re-generate scientific findings.
@article{castleberry2013,
          author    = {Dennis Castleberry}
         ,title     = {Inkling: An Executable Paper System for Reviewing Scientific Applications}
         ,journal   = {SocialCom}
         ,address   = {Washington, D.C.}
         ,publisher = {IEEE}
         ,year      = {2013}
}


The Piraha language
@article{piraha,
          author    = {Steven R Brandt}
         ,title     = {Piraha: A simplified grammar parser for component little languages}
         ,journal   = {Grid Computing}
         ,address   = {Brussels, Belgium}
         ,publisher = {IEEE}
         ,year      = {2010}
}


Ebbinghaus original paper
@article{ebbinghaus,
          author    = {Hermann Ebbinghaus}
         ,title     = {{\"U}ber das Ged{\"a}chti{\ss}}
         ,journal   = {Passavia Universit{\"a}tsverlag}
         ,year      = {1880}
}


The one and only original ACT-R paper
@article{anderson1996act,
  title={ACT: A simple theory of complex cognition.},
  author={Anderson, John R},
  journal={American Psychologist},
  volume={51},
  number={4},
  pages={355},
  year={1996},
  publisher={American Psychological Association}
}


This is where many of the later equations come from
@article{anderson2000implications,
  title={Implications of the ACT-R learning theory: No magic bullets},
  author={Anderson, John R and Schunn, C},
  journal={Advances in instructional psychology, Educational design and cognitive science},
  pages={1--33},
  year={2000}
}


ACT-R can make predictions about time to search a menu.
@article{anderson1997act,
  title={ACT-R: A theory of higher level cognition and its relation to visual attention},
  author={Anderson, John R and Matessa, Michael and Lebiere, Christian},
  journal={Human-Computer Interaction},
  volume={12},
  number={4},
  pages={439--462},
  year={1997},
  publisher={L. Erlbaum Associates Inc.}
}


The original logistic regression paper
@article{cox1958regression,
  title={The regression analysis of binary sequences},
  author={Cox, David R},
  journal={Journal of the Royal Statistical Society. Series B (Methodological)},
  pages={215--242},
  year={1958},
  publisher={JSTOR}
}



...26 studies were deemed as relevant. The main findings from these studies
are: i) Bloom’s taxonomy has mostly been applied at undergraduate level for
both design and assessment of software engineering courses; ii) software
construction is the leading SE subarea in which Bloom’s taxonomy has been
applied. The results clearly point out the usefulness of Bloom’s taxonomy in
the SE education context.
@article{ britto2015,
   author = {Ricardo Britto and Muhammad Usman}
  ,title  = {Bloom’s Taxonomy in Software Engineering Education: A Systematic Mapping Study}
  ,journal= {Frontiers in Education Conference}
  ,address= {El Paso, TX}
  ,publisher= {IEEE}
  ,year   = {2015}
}


Feedback from the students from both first year and second year reflected a
positive response concerning their perception of the level on which the
questions were asked. Regarding the first question, 381 students felt that by
asking questions on higher levels of Bloom’s taxonomy it enriched their
learning
@article{bruyn2011,
   author = {E de Bruyn and E Mostert and A van Schoor}
  ,title  = {Computer-based testing--the ideal tool to assess on the different levels of Bloom’s taxonomy}
  ,journal= {Interactive Collaborative Learning}
  ,address= {Piesany, Slovakia}
  ,publisher= {IEEE}
  ,year   = {2011}
}

Bloom's taxonomy as applied to program comprehension was successful
@article{buckley2003,
   author = {Jim Buckley and Chris Exton}
  ,title  = {Blooms’ Taxonomy: A Framework for Assessing Programmers’ Knowledge of Software Systems}
  ,journal= {International Workshop on Program Comprehension}
  ,address= {}
  ,publisher= {IEEE}
  ,year   = {2003}
}

Clustering of tasks using EEG is possible
@article{chatterjee2015,
   author = {Debatri Chatterjee and rajat Das and Anirhudda Sinha and Shreyasi Datta}
  ,title  = {Analyzing Elementary Cognitive Tasks with Bloom's Taxonomy using Low Cost Commercial EEG device} 
  ,journal= {International Conference on Intelligent Sensors, Sensor Networks and Information Processing (ISSNIP)}
  ,address= {}
  ,publisher= {IEEE}
  ,year   = {2015}
}

Preference for higher-order cognitive activities
@article{ goel2004,
   author = {Sanjay Goel and Nalin Sharda}
  ,title  = {What do engineers want? Examining engineering education through Bloom's taxonomy}
  ,journal= {Australasian Association for Engineering Education}
  ,address= {}
  ,publisher= {}
  ,year   = {2004}
}


Context-aware vs. lexical schema
Successful application to system
 @article{ kelly2006,
    author = {Tara Kelly and Jim Buckley}
   ,title  = {A Context-Aware Analysis Scheme for Bloom's Taxonomy}
   ,journal= {International Conference on Program Comprehension}
   ,address= {}
   ,publisher= {IEEE}
   ,year   = {2006}
 }

several points aobut requirements
 formal language
 typing system
 Bloom's semantics
@article{loria-saenz2008,
   author = {Carlos Loria-Saenz}
  ,title  = {On Requirements for Programming Exercises from an e-Learning Perspective}
  ,journal= {}
  ,address= {}
  ,publisher= {}
  ,year   = {2008}
}

increase in motivation
comprehension testing fosters code reading
@article{losada2008,
   author = {Isidoro Hern\'{a}n-Losada}
  ,title  = {Testing-Based Automatic Grading: A Proposal from Bloom’s Taxonomy}
  ,journal= {International Conference on Advanced Learning Technologies}
  ,address= {}
  ,publisher= {IEEE}
  ,year   = {2008}
}

speaks well for bloom's taxonomy when manually implemented
@article{mahmood2014,
   author = {Mahmood Niazi}
  ,title  = {Teaching global software engineering: experiences and lessons learned}
  ,journal= {IET Software}
  ,address= {}
  ,publisher= {}
  ,year   = {2014}
}

good for novice programmers to dial it down
@article{shuhidan2011,
   author = {Shuhaida Shuhidan and Margaret Hamilton and Daryl D'Souza }
  ,title  = { Understanding Novice Programmer Difficulties via Guided Learning }
  ,journal= {ITiCSE}
  ,address= {Darmstadt, Germany}
  ,publisher= {ACM}
  ,year   = {2011}
}


My last paper
@inproceedings{castleberry2016effect,
  title={The Effect of Question Ordering Using Bloom's Taxonomy in an e-Learning Environment},
  author={Castleberry, Dennis and Brandt, Steven R},
  booktitle={International Conference on Computer Science Education Innovation \& Technology (CSEIT). Proceedings},
  pages={22},
  year={2016},
  organization={Global Science and Technology Forum}
}



speaks well for use of Bloom's taxonomy in software inspection
@article{ mcmeekin2009
  ,author = {David McMeekin and Brian von Konsky and Elizabeth Chang and David Cooper}
  ,title  = { }
  ,journal= { Conference on Software Engineering Education and Training }
  ,address= {}
  ,publisher= {IEEE}
  ,year   = {2009}
}

----------------------------------------------------------------------------

Attempt to identify production rules from response set 
@article{tatsuoka1990toward,
  title={Toward an integration of item-response theory and cognitive error diagnosis},
  author={Tatsuoka, Kikumi K},
  journal={Diagnostic monitoring of skill and knowledge acquisition},
  pages={453--488},
  year={1990}
}

Both professional certification and academic tests rely heavily on
multiple-choice questions, despite the widespread belief that alternate,
constructed-response questions are superior measures of a test taker's
understanding of the underlying material. Empirically, the search for a link
between these two assessment metrics has met with limited success, leading some
researchers to conclude that the relationship is close and others to conclude
that no relationship exists at all. The authors suggest that “knowledge level”
may play a key role in explaining this disparity in findings. This article
outlines the theory for such a concept, and investigates the possibility using
172 carefully constructed tests in several entry-level programming classes. The
article also discusses several caveats that suggest the usefulness of yet
further research in the area.
@article{kuechler2010performance,
  title={Why Is Performance on Multiple-Choice Tests and Constructed-Response Tests Not More Closely Related? Theory and an Empirical Test},
  author={Kuechler, William L and Simkin, Mark G},
  journal={Decision Sciences Journal of Innovative Education},
  volume={8},
  number={1},
  pages={55--73},
  year={2010},
  publisher={Wiley Online Library}
}
0

This study introduces the development of a Web-based assessment system, the
Web-based Assessment and Test Analyses (WATA) system, and examines its impacts
on teacher education. The WATA system is a follow-on system, which applies the
Triple-A Model (assembling, administering, and appraising). Its functions
include (1) an engine for teachers to administer and manage testing, (2) an
engine for students to apply tests, and (3) an engine for generating test
results and analyses for teachers. Two studies were undertaken to assess the
usefulness and potential benefits of the WATA system for teacher education. In
the first study, 47 in-service teachers were asked to assess the functions of
the WATA system. The results indicated that they were satisfied with the
Triple-A Model of the WATA system. In the second study, 30 pre-service teachers
were required to use the WATA system during the teacher-training program. After
4 months of experience in using the WATA system, the pre-service teachers'
perspectives of assessment have been changed significantly. The findings of
these two studies might provide some guidance to help those who are interested
in the development of Web-based assessment and intend to infuse information
technology into teacher education.
@article{wang2004web,
  title={Web-based Assessment and Test Analyses (WATA) system: development and evaluation},
  author={Wang, Tzu-Hua and Wang, Kuo-Hua and Wang, Wei-Lung and Huang, Shih-Chieh and Chen, Sherry Y},
  journal={Journal of Computer Assisted Learning},
  volume={20},
  number={1},
  pages={59--71},
  year={2004},
  publisher={Wiley Online Library}
}
0

Pressure for better measurement of stated learning outcomes has resulted in a
demand for more frequent assessment. The resources available are seen to be
static or dwindling, but Information and Communications Technology is seen to
increase productivity by automating assessment tasks. This paper reviews
computer-assisted assessment (CAA) and suggests future developments. A search
was conducted of CAA-related literature from the past decade to trace the
development of CAA from the beginnings of its large-scale use in higher
education. Lack of resources, individual inertia and risk propensity are key
barriers for individual academics, while proper resourcing and cultural factors
outweigh technical barriers at the institutional level.
@article{conole2005review,
  title={A review of computer-assisted assessment},
  author={Conole, Gr{\'a}inne and Warburton, Bill},
  journal={ALT-J},
  volume={13},
  number={1},
  pages={17--31},
  year={2005},
  publisher={Taylor \& Francis}
}
0

Present students' evaluation measurement generally practiced in Institutions of
Higher Learning (IHL) is largely dependent on students' performance in carrying
out tasks such as a series of tests or quizzes, final examination and
submission of assignments. However, the over use of Cumulative Grade Point
Average (CGPA) which computation is purely the mean of raw scores, lacks
precision and linearity hence validity required to meet the fundamental
criteria of measurement warrants a review. In IHL, the theory and practice of
classical test theory, the traditional approach of students' evaluation must be
re-assessed. This paper provides an overview of an alternative “modern”
measurement as practiced using item response theory with focus on Rasch
measurement model. An overview of Rasch measurement model and its key concepts
are presented. This assessment model proved to be a better students' assessment
method and can be used to validate the CLO of each course. A case study in the
College of Engineering, Umm al-Qura', Makkah on students (N=75) was conducted
to measure their command of knowledge (Kn) and understanding (uN) for the
subject 804431 -Manufacturing Process III; as categorised according to Bloom's
Taxonomy disclosed that there is a need to take immediate action where their
μPerson measure level is disturbingly low; μperson = −0.29logit. μitem
measurement also indicates that the undergraduates encounter significant
difficulties in grasping some fundamental engineering principles in extrusion.
Only 30.67% (N=23) of the students measured found to be above the itemmeasure
≥0.00 logit. Whilst 6.67% (Nitem=5) of the students were found to be below the
Minimumitem measure ≤ −1.03logit. The study shows that SPELA as a model of
measurement can provide better estimate of students' ability more accurately
based on the CLO as compared to the traditional CGPA method of assessment using
raw score.  Present students' evaluation measurement generally practiced in
@inproceedings{ghulman2009modern,
  title={Modern measurement paradigm in Engineering Education: Easier to read and better analysis using Rasch-based approach},
  author={Ghulman, Hamzah A and Mas' odi, Mohd Saidfudin},
  booktitle={Engineering Education (ICEED), 2009 International Conference on},
  pages={1--6},
  year={2009},
  organization={IEEE}
}
0

Distinguishes between error analysis and deficit assessment
@article{bejar1984educational,
  title={Educational diagnostic assessment},
  author={Bejar, Isaac I},
  journal={Journal of educational measurement},
  pages={175--189},
  year={1984},
  publisher={JSTOR}
}
0

@article{osborne2013grounded,
  title={The grounded psychometric development and initial validation of the Health Literacy Questionnaire (HLQ)},
  author={Osborne, Richard H and Batterham, Roy W and Elsworth, Gerald R and Hawkins, Melanie and Buchbinder, Rachelle},
  journal={BMC public health},
  volume={13},
  number={1},
  pages={658},
  year={2013},
  publisher={BioMed Central}
}
0

The purpose of this paper is to define and evaluate the categories of cognitive
models underlying at least three types of educational tests. We argue that
while all educational tests may be based—explicitly or implicitly—on a
cognitive model, the categories of cognitive models underlying tests often
range in their development and in the psychological evidence gathered to
support their value. For researchers and practitioners, awareness of different
cognitive models may facilitate the evaluation of educational measures for the
purpose of generating diagnostic inferences, especially about examinees'
thinking processes, including misconceptions, strengths, and/or abilities. We
think a discussion of the types of cognitive models underlying educational
measures is useful not only for taxonomic ends, but also for becoming
increasingly aware of evidentiary claims in educational assessment and for
promoting the explicit identification of cognitive models in test development.
We begin our discussion by defining the term cognitive model in educational
measurement. Next, we review and evaluate three categories of cognitive models
that have been identified for educational testing purposes using examples from
the literature. Finally, we highlight the practical implications of “blending”
models for the purpose of improving educational measures.
 @article{leighton2007defining,
   title={Defining and evaluating models of cognition used in educational measurement to make inferences about examinees' thinking processes},
   author={Leighton, Jacqueline P and Gierl, Mark J},
   journal={Educational Measurement: Issues and Practice},
   volume={26},
   number={2},
   pages={3--16},
   year={2007},
   publisher={Wiley Online Library}
 }


This study carefully examines two widely used methods for assessing marketing
learning outcomes: multiple-choice tests and short-answer tests. Student scores
on multiple-choice and short-answer portions of a midterm exam in consumer
behavior were compared in terms of time to completion, reliability, and
validity. The multiple-choice format was found to yield equivalent reliability
and validity in a shorter amount of test-taking time. In contrast to some
earlier studies, no gender effect was found: neither format differentially
benefited men or women.
@article{bacon2003assessing,
  title={Assessing learning outcomes: A comparison of multiple-choice and short-answer questions in a marketing context},
  author={Bacon, Donald R},
  journal={Journal of Marketing Education},
  volume={25},
  number={1},
  pages={31--36},
  year={2003},
  publisher={Sage Publications}
}


