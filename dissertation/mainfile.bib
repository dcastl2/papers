


% Duplicate of my research, more importantly suggests outright that Bloom=difficulty
@article{newman1988effect,
  title={Effect of varying item order on multiple-choice test scores: Importance of statistical and cognitive difficulty},
  author={Newman, Dianna L and Kundert, Deborah K and Lane Jr, David S and Bull, Kay Sather},
  journal={Applied Measurement in education},
  volume={1},
  number={1},
  pages={89--97},
  year={1988},
  publisher={Taylor \& Francis}
}


% Very clear support for the thought that Bloom=difficulty, even used for course difficulty rating
@inproceedings{oliver2004course,
  title={This course has a Bloom Rating of 3.9},
  author={Oliver, Dave and Dobele, Tony and Greber, Myles and Roberts, Tim},
  booktitle={Proceedings of the Sixth Australasian Conference on Computing Education-Volume 30},
  pages={227--231},
  year={2004},
  organization={Australian Computer Society, Inc.}
}


% Very clear support for the thought that Bloom=difficulty
@article{lord2007moving,
  title={Moving students from information recitation to information understanding: exploiting Bloom's taxonomy in creating science questions},
  author={Lord, Thomas and Baviskar, Sandhya},
  journal={Journal of College Science Teaching},
  volume={36},
  number={5},
  pages={40},
  year={2007},
  publisher={National Science Teachers Association}
}


% Support for people thinking that Bloom=difficulty
@inproceedings{johnson2006bloom,
  title={Is Bloom's taxonomy appropriate for computer science?},
  author={Johnson, Colin G and Fuller, Ursula},
  booktitle={Proceedings of the 6th Baltic Sea conference on Computing education research: Koli Calling 2006},
  pages={120--123},
  year={2006},
  organization={ACM}
}


% Support for people thinking that Bloom=difficulty
@inproceedings{fuller2007developing,
  title={Developing a computer science-specific learning taxonomy},
  author={Fuller, Ursula and Johnson, Colin G and Ahoniemi, Tuukka and Cukierman, Diana and Hern{\'a}n-Losada, Isidoro and Jackova, Jana and Lahtinen, Essi and Lewis, Tracy L and Thompson, Donna McGee and Riedesel, Charles et al.},
  booktitle={ACM SIGCSE Bulletin},
  volume={39:4},
  pages={152--170},
  year={2007},
  organization={ACM}
}


% Support for Bloom!=difficulty: facility versus complexity
@article{hill1981testing,
  title={Testing the simplex assumption underlying Bloom's Taxonomy},
  author={Hill, PW and McGaw, B},
  journal={American Educational Research Journal},
  volume={18},
  number={1},
  pages={93--101},
  year={1981},
  publisher={Sage Publications}
}


% Support for Bloom!=difficulty
@inproceedings{thompson2008bloom,
  title={Bloom's taxonomy for CS assessment},
  author={Thompson, Errol and Luxton-Reilly, Andrew and Whalley, Jacqueline L and Hu, Minjie and Robbins, Phil},
  booktitle={Proceedings of the tenth conference on Australasian computing education-Volume 78},
  pages={155--161},
  year={2008},
  organization={Australian Computer Society, Inc.}
}


% Using item response theory (IRT) in computer adaptive test (CAT) is critical to
% assess learners' readiness for further learning. This theory has assumptions
% concerning the mathematical relationship between learner's abilities and item
% responses. A numerical value from the theory has potential to decide who the
% best learner is. The IRT score only provides the learner's result and lack of
% the assessment of learner's ability level in cognitive skills. This paper
% proposes an approach of using Bloom's cognitive skills for evaluating learned
% ability level by considering achieved the level of difficulty and the IRT
% score. Our starting point in the adaptive test is at a question of learners'
% knowledge level difficulty that we got from their pre-test scores instead of
% starting with a question of a medium difficulty. This approach provides the
% effectiveness of CAT to test cognitive skills and offers some theoretical
% considerations on linking learning outcomes and assessments.
@article{sitthisak,
  title={Cognitive Assessment Applying with Item Response Theory},
  author={Sitthisak, Onjira and Soonklang, Tasanawan and Gilbert, Lester},
  journal={19th Annual International Conference on Computers in Education},
  year={2011}
}


% Bloom original
@book{bloom1956,
  title={Taxonomy of educational objectives},
  author={Bloom, Benjamin Samuel et al.},
  year={1956},
  publisher={David McKay}
}


% Newton Raphson
@book{baker2004,
  title={Item response theory: Parameter estimation techniques},
  author={Baker, Frank B and Kim, Seock-Ho},
  year={2004},
  publisher={CRC Press}
}


% R can be hooked to paper manuscripts to re-generate scientific findings.
@article{ castleberry2011,
          author    = {Dennis Castleberry}
         ,title     = {The Prickly Pear Archive}
         ,journal   = {ICCS}
         ,address   = {Singapore}
         ,publisher = {Elsevier}
         ,year      = {2011}
}


% ...26 studies were deemed as relevant. The main findings from these studies
% are: i) Bloom’s taxonomy has mostly been applied at undergraduate level for
% both design and assessment of software engineering courses; ii) software
% construction is the leading SE subarea in which Bloom’s taxonomy has been
% applied. The results clearly point out the usefulness of Bloom’s taxonomy in
% the SE education context.
@article{ britto2015,
   author = {Ricardo Britto and Muhammad Usman}
  ,title  = {Bloom’s Taxonomy in Software Engineering Education: A Systematic Mapping Study}
  ,journal= {Frontiers in Education Conference}
  ,address= {El Paso, TX}
  ,publisher= {IEEE}
  ,year   = {2015}
}


% Feedback from the students from both first year and second year reflected a
% positive response concerning their perception of the level on which the
% questions were asked. Regarding the first question, 381 students felt that by
% asking questions on higher levels of Bloom’s taxonomy it enriched their
% learning
@article{ bruyn2011
  ,author = {E de Bruyn and E Mostert and A van Schoor}
  ,title  = {Computer-based testing--the ideal tool to assess on the different levels of Bloom’s taxonomy}
  ,journal= {Interactive Collaborative Learning}
  ,address= {Piesany, Slovakia}
  ,publisher= {IEEE}
  ,year   = {2011}
}

% Bloom's taxonomy as applied to program comprehension was successful
@article{ buckley2003
  ,author = {Jim Buckley and Chris Exton}
  ,title  = {Blooms’ Taxonomy: A Framework for Assessing Programmers’ Knowledge of Software Systems}
  ,journal= {International Workshop on Program Comprehension}
  ,address= {}
  ,publisher= {IEEE}
  ,year   = {2003}
}

% Clustering of tasks using EEG is possible
@article{ chatterjee2015
  ,author = {Debatri Chatterjee and rajat Das and Anirhudda Sinha and Shreyasi Datta}
  ,title  = {Analyzing Elementary Cognitive Tasks with Bloom's Taxonomy using Low Cost Commercial EEG device} 
  ,journal= {International Conference on Intelligent Sensors, Sensor Networks and Information Processing (ISSNIP)}
  ,address= {}
  ,publisher= {IEEE}
  ,year   = {2015}
}

% Preference for higher-order cognitive activities
@article{ goel2004,
   author = {Sanjay Goel and Nalin Sharda}
  ,title  = {What do engineers want? Examining engineering education through Bloom's taxonomy}
  ,journal= {Australasian Association for Engineering Education}
  ,address= {}
  ,publisher= {}
  ,year   = {2004}
}


% Context-aware vs. lexical schema
% Successful application to system
@article{ kelly2006,
   author = {Tara Kelly and Jim Buckley}
  ,title  = {A Context-Aware Analysis Scheme for Bloom's Taxonomy}
  ,journal= {International Conference on Program Comprehension}
  ,address= {}
  ,publisher= {IEEE}
  ,year   = {2006}
}

% several points aobut requirements
%  formal language
%  typing system
%  Bloom's semantics
@article{ loria-saenz2008,
   author = {Carlos Loria-Saenz}
  ,title  = {On Requirements for Programming Exercises from an e-Learning Perspective}
  ,journal= {}
  ,address= {}
  ,publisher= {}
  ,year   = {2008}
}

% increase in motivation
% comprehension testing fosters code reading
@article{ losada2008
  ,author = {Isidoro Hern\'{a}n-Losada}
  ,title  = {Testing-Based Automatic Grading: A Proposal from Bloom’s Taxonomy}
  ,journal= {International Conference on Advanced Learning Technologies}
  ,address= {}
  ,publisher= {IEEE}
  ,year   = {2008}
}

% speaks well for bloom's taxonomy when manually implemented
@article{ mahmood2014
  ,author = {Mahmood Niazi}
  ,title  = {Teaching global software engineering: experiences and lessons learned}
  ,journal= {IET Software}
  ,address= {}
  ,publisher= {}
  ,year   = {2014}
}

% good for novice programmers to dial it down
@article{ shuhidan2011
  ,author = {Shuhaida Shuhidan and Margaret Hamilton and Daryl D'Souza }
  ,title  = { Understanding Novice Programmer Difficulties via Guided Learning }
  ,journal= {ITiCSE}
  ,address= {Darmstadt, Germany}
  ,publisher= {ACM}
  ,year   = {2011}
}


% My last paper
@inproceedings{castleberry2016,
  title={The Effect of Question Ordering Using Bloom's Taxonomy in an e-Learning Environment},
  author={Castleberry, Dennis and Brandt, Steven R},
  booktitle={International Conference on Computer Science Education Innovation \& Technology (CSEIT). Proceedings},
  pages={22},
  year={2016},
  organization={Global Science and Technology Forum}
}



% speaks well for use of Bloom's taxonomy in software inspection
@article{ mcmeekin2009
  ,author = {David McMeekin and Brian von Konsky and Elizabeth Chang and David Cooper}
  ,title  = { }
  ,journal= { Conference on Software Engineering Education and Training }
  ,address= {}
  ,publisher= {IEEE}
  ,year   = {2009}
}

% ----------------------------------------------------------------------------

Attempt to identify production rules from response set 
@article{tatsuoka1990toward,
  title={Toward an integration of item-response theory and cognitive error diagnosis},
  author={Tatsuoka, Kikumi K},
  journal={Diagnostic monitoring of skill and knowledge acquisition},
  pages={453--488},
  year={1990}
}

Both professional certification and academic tests rely heavily on
multiple-choice questions, despite the widespread belief that alternate,
constructed-response questions are superior measures of a test taker's
understanding of the underlying material. Empirically, the search for a link
between these two assessment metrics has met with limited success, leading some
researchers to conclude that the relationship is close and others to conclude
that no relationship exists at all. The authors suggest that “knowledge level”
may play a key role in explaining this disparity in findings. This article
outlines the theory for such a concept, and investigates the possibility using
172 carefully constructed tests in several entry-level programming classes. The
article also discusses several caveats that suggest the usefulness of yet
further research in the area.
@article{kuechler2010performance,
  title={Why Is Performance on Multiple-Choice Tests and Constructed-Response Tests Not More Closely Related? Theory and an Empirical Test},
  author={Kuechler, William L and Simkin, Mark G},
  journal={Decision Sciences Journal of Innovative Education},
  volume={8},
  number={1},
  pages={55--73},
  year={2010},
  publisher={Wiley Online Library}
}
0

This study introduces the development of a Web-based assessment system, the
Web-based Assessment and Test Analyses (WATA) system, and examines its impacts
on teacher education. The WATA system is a follow-on system, which applies the
Triple-A Model (assembling, administering, and appraising). Its functions
include (1) an engine for teachers to administer and manage testing, (2) an
engine for students to apply tests, and (3) an engine for generating test
results and analyses for teachers. Two studies were undertaken to assess the
usefulness and potential benefits of the WATA system for teacher education. In
the first study, 47 in-service teachers were asked to assess the functions of
the WATA system. The results indicated that they were satisfied with the
Triple-A Model of the WATA system. In the second study, 30 pre-service teachers
were required to use the WATA system during the teacher-training program. After
4 months of experience in using the WATA system, the pre-service teachers'
perspectives of assessment have been changed significantly. The findings of
these two studies might provide some guidance to help those who are interested
in the development of Web-based assessment and intend to infuse information
technology into teacher education.
@article{wang2004web,
  title={Web-based Assessment and Test Analyses (WATA) system: development and evaluation},
  author={Wang, Tzu-Hua and Wang, Kuo-Hua and Wang, Wei-Lung and Huang, Shih-Chieh and Chen, Sherry Y},
  journal={Journal of Computer Assisted Learning},
  volume={20},
  number={1},
  pages={59--71},
  year={2004},
  publisher={Wiley Online Library}
}
0

Pressure for better measurement of stated learning outcomes has resulted in a
demand for more frequent assessment. The resources available are seen to be
static or dwindling, but Information and Communications Technology is seen to
increase productivity by automating assessment tasks. This paper reviews
computer-assisted assessment (CAA) and suggests future developments. A search
was conducted of CAA-related literature from the past decade to trace the
development of CAA from the beginnings of its large-scale use in higher
education. Lack of resources, individual inertia and risk propensity are key
barriers for individual academics, while proper resourcing and cultural factors
outweigh technical barriers at the institutional level.
@article{conole2005review,
  title={A review of computer-assisted assessment},
  author={Conole, Gr{\'a}inne and Warburton, Bill},
  journal={ALT-J},
  volume={13},
  number={1},
  pages={17--31},
  year={2005},
  publisher={Taylor \& Francis}
}
0

Present students' evaluation measurement generally practiced in Institutions of
Higher Learning (IHL) is largely dependent on students' performance in carrying
out tasks such as a series of tests or quizzes, final examination and
submission of assignments. However, the over use of Cumulative Grade Point
Average (CGPA) which computation is purely the mean of raw scores, lacks
precision and linearity hence validity required to meet the fundamental
criteria of measurement warrants a review. In IHL, the theory and practice of
classical test theory, the traditional approach of students' evaluation must be
re-assessed. This paper provides an overview of an alternative “modern”
measurement as practiced using item response theory with focus on Rasch
measurement model. An overview of Rasch measurement model and its key concepts
are presented. This assessment model proved to be a better students' assessment
method and can be used to validate the CLO of each course. A case study in the
College of Engineering, Umm al-Qura', Makkah on students (N=75) was conducted
to measure their command of knowledge (Kn) and understanding (uN) for the
subject 804431 -Manufacturing Process III; as categorised according to Bloom's
Taxonomy disclosed that there is a need to take immediate action where their
μPerson measure level is disturbingly low; μperson = −0.29logit. μitem
measurement also indicates that the undergraduates encounter significant
difficulties in grasping some fundamental engineering principles in extrusion.
Only 30.67% (N=23) of the students measured found to be above the itemmeasure
≥0.00 logit. Whilst 6.67% (Nitem=5) of the students were found to be below the
Minimumitem measure ≤ −1.03logit. The study shows that SPELA as a model of
measurement can provide better estimate of students' ability more accurately
based on the CLO as compared to the traditional CGPA method of assessment using
raw score.  Present students' evaluation measurement generally practiced in
@inproceedings{ghulman2009modern,
  title={Modern measurement paradigm in Engineering Education: Easier to read and better analysis using Rasch-based approach},
  author={Ghulman, Hamzah A and Mas' odi, Mohd Saidfudin},
  booktitle={Engineering Education (ICEED), 2009 International Conference on},
  pages={1--6},
  year={2009},
  organization={IEEE}
}
0

% Distinguishes between error analysis and deficit assessment
@article{bejar1984educational,
  title={Educational diagnostic assessment},
  author={Bejar, Isaac I},
  journal={Journal of educational measurement},
  pages={175--189},
  year={1984},
  publisher={JSTOR}
}
0

@article{osborne2013grounded,
  title={The grounded psychometric development and initial validation of the Health Literacy Questionnaire (HLQ)},
  author={Osborne, Richard H and Batterham, Roy W and Elsworth, Gerald R and Hawkins, Melanie and Buchbinder, Rachelle},
  journal={BMC public health},
  volume={13},
  number={1},
  pages={658},
  year={2013},
  publisher={BioMed Central}
}
0

The purpose of this paper is to define and evaluate the categories of cognitive
models underlying at least three types of educational tests. We argue that
while all educational tests may be based—explicitly or implicitly—on a
cognitive model, the categories of cognitive models underlying tests often
range in their development and in the psychological evidence gathered to
support their value. For researchers and practitioners, awareness of different
cognitive models may facilitate the evaluation of educational measures for the
purpose of generating diagnostic inferences, especially about examinees'
thinking processes, including misconceptions, strengths, and/or abilities. We
think a discussion of the types of cognitive models underlying educational
measures is useful not only for taxonomic ends, but also for becoming
increasingly aware of evidentiary claims in educational assessment and for
promoting the explicit identification of cognitive models in test development.
We begin our discussion by defining the term cognitive model in educational
measurement. Next, we review and evaluate three categories of cognitive models
that have been identified for educational testing purposes using examples from
the literature. Finally, we highlight the practical implications of “blending”
models for the purpose of improving educational measures.
@article{leighton2007defining,
  title={Defining and evaluating models of cognition used in educational measurement to make inferences about examinees' thinking processes},
  author={Leighton, Jacqueline P and Gierl, Mark J},
  journal={Educational Measurement: Issues and Practice},
  volume={26},
  number={2},
  pages={3--16},
  year={2007},
  publisher={Wiley Online Library}
}
0

This study carefully examines two widely used methods for assessing marketing
learning outcomes: multiple-choice tests and short-answer tests. Student scores
on multiple-choice and short-answer portions of a midterm exam in consumer
behavior were compared in terms of time to completion, reliability, and
validity. The multiple-choice format was found to yield equivalent reliability
and validity in a shorter amount of test-taking time. In contrast to some
earlier studies, no gender effect was found: neither format differentially
benefited men or women.
@article{bacon2003assessing,
  title={Assessing learning outcomes: A comparison of multiple-choice and short-answer questions in a marketing context},
  author={Bacon, Donald R},
  journal={Journal of Marketing Education},
  volume={25},
  number={1},
  pages={31--36},
  year={2003},
  publisher={Sage Publications}
}
0

