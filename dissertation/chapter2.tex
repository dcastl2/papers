\section{Representing Items}


The database contains items of the following nature:

\begin{itemize}

  \item \emph{Content}.  These include facts (which may be arranged into
  paragraphs, subsections, and sections), definitions, diagrams, and source
  codes.

  \item \emph{Assessment}.  These include questions, which may be true/false,
  multiple-choice, code writing, code simulation, short answer, and
  freewriting; also Likert scale items.

\end{itemize}

\subsection{Taxonomic Information}

Any output from this database to the student which is intended to solicit input
from the student has the following ordinal dimensions:

\begin{itemize}

  \item \emph{Difficulty}. Following the +/- grading system, difficulties range
  from -7 (very easy) to +7 (very hard), with 0 being medium. The difficulty
  indicates the probability that any given student in the class will answer the
  item correctly.

  \item \emph{Bloom level}.  The Bloom levels of cognitive functioning are
  Knowledge, Comprehension, Application, Analysis, Evaluation, and Synthesis.

  \item \emph{Concept}.  Concepts are covered in an elementary C++
  programming course, and include: Variables, Expressions, Control Structures,
  etc.

\end{itemize}

Any output to the student also has the following categorial dimensions:

\begin{itemize}

  \item \emph{Context}.  A problem may have a domain-specific context, e.g. it
  may be a problem relevant to biology, chemistry, physics, mathematics, etc.

  \item \emph{Type}.  The problem may be true/false, multiple-choice, code
  writing, code simulation, short answer, or freewriting.

\end{itemize}

Each output may also have a dependency list, that is a list of IDs of, or a
rule describing, other output entries which the student should be exposed to
prior to that output.  It is important to note that the dimensions of the data
are not limited to the above; an exploratory factor analysis has the potential
to extend the dimensionality of the data semi-automatically.  For the moment,
Bloom level, subject domain, concept, and difficulty are dimensions of a test
question.  Exploratory factor analysis gives loadings of some $n$ number of
factors which are hypothesized to account for the variance in the data.  Thus
it could potentially find a factor other than one listed which accounts for a
significant amount of variance. In that case, the instructor could interpret
the loading matrix and label the levels of the factor accordingly.

\subsubsection{Examples of Content Items}

What follows is example output which does not aim to solicit input, but rather
inform the user (content).  Below is a definition of a compiler output.

\begin{quote}
[Definition 1]: compiler: a program which generates an executable from a source
code
\end{quote}

Likewise, below is a fact which has as one of its dependencies the definition
of a compiler, given above.

\begin{quote}
[Fact 1]: A compiler is itself a program.
\end{quote}

In a content schedule, Fact 1 has Definition 1 as a dependency. Therefore in
any schedule including Fact 1, Definition 1 will be included unless an
assessment response set shows that Definition 1 is known.

\subsubsection{Measuring Trait Ability}

To form a statistical basis for content scheduling, the measure of trait
ability should be done per-concept and Bloom level.  For each $i^{th}$
question, the trait ability estimation should take into account:

\begin{itemize}

  \item the probability of guessing the question correctly, $\gamma_i$,

  \item the item discrimination $\alpha_i$, or to what extent the question
        distinguishes good from poor trait ability for the concept,

  \item the difficulty $\beta_i$, including the question difficulty \emph{as
        such}

  \item the propensity of trait level to change over time.

  \end{itemize}

Trait ability will be measured using (a possibly modified form of) IRT.  In
contrast to CTT---in which the meaning of a composite test score is interpreted
by comparing it to the mean of composite scores obtained from the same
sample--- IRT obtains meaning of test results by computing the likelihood that
the student could answer the questions in the way he or she did assuming a
trait ability.  In CTT, for a standardized test, one may calculate the z-score
for a composite test score as follows:

\[
  z = \frac{x - \mu}{\sigma}.
\]
%\todo{Is z-score a per trait thing? a per bloom level thing?}
%\response{The z-score is calculated from composite test scores. It tells how
%many standard deviations above or below the mean a composite score is.}

The $z$-score gives the number of standard deviations from the mean.  $-1 < z <
l$ average, $z > 1$ is above average, and $z < -1$ is below average.  One may
score the test accordingly.  Some instructors use a more intuitive approach,
graphing the distribution and clustering the data, to identify distinct
clusters of scores.  However, neither measure of trait ability accounts for the
considerations above (unless item discrimination was ensured by using a factor
analysis to design the test). 

In IRT, the trait ability is measured using a formula expressing the
probability that the student $s$ will answer the item $i$ correctly
\cite{embretson2000}: 

\[
 P(X_{is}=1 | \theta_s, \alpha_i, \beta_i, \gamma_i) = 
 \gamma_i + (1-\gamma_i) 
 \frac{    e^{\alpha_{i}(\theta_{s} - \beta_{i})}}
      {1 + e^{\alpha_{i}(\theta_{s} - \beta_{i})}}
\]

Suppose the student's response vector is $y_s$, where $y_{is}$ is the student's
response to the $i^{th}$ question.  $y_{is}$ is 0 if the answer is incorrect,
and 1 otherwise.  If the trait ability $\theta_s$ is not known, then it may be
estimated by searching the hypothesis space of $\theta_s$ for the probability
which maximizes $\prod^m P(y_{is})$, that is the product of probabilities that
$s$ answers each of the $m$ total questions for that question in the manner
he or she did.  That is, we seek the $\theta_s$ which maximizes the probility
of the student's response set:

\[
 \underset{\theta_s}{\textrm{argmax}}
 \prod^m
 \Bigg[
 \gamma_i + (1-\gamma_i) 
 \frac{    e^{\alpha_{i}(\theta_{s} - \beta_{i})}}
      {1 + e^{\alpha_{i}(\theta_{s} - \beta_{i})}}
 \Bigg].
\]

To find this trait ability, we may use a gradient descent method on the above
function of $\theta_s$ until convergence to a satisfactory precision required
to assign a letter grade (1e-1) \cite{embretson2000}. 

\subsection{Dependency Information}

\section{Representing Assessments}

\subsection{Graph Structure}
