
\section{Introduction}

\subsection{Problem Definition}

A \emph{content item} is some unit of information, such as a statement, graph,
table, example, and so forth.  An educationl television program, interactive
tutorial or university lecture (generally, these may all be called programs)
can be broken down into these units of information.  Call an item $\chi$; then
the $i^{th}$ item to be seen is $\chi_i$, and the time at which it is seen
$t_i$.  Then $(\chi_i, t_i)$ represents the $i^{th}$ item and its scheduled
time.  

For that matter, any program can be thought of as a sequence of these tuples,
thereby forming a schedule of items: 

\begin{equations}
\label{eq:schedule}
   X = \langle (\chi_1, t_1), (\chi_2, t_2), \ldots (\chi_n, t_n) \rangle.
\end{equations}
\vspace{2pt}

Furthermore $\chi$ could be a question, which could be thought of as an item
which accepts a response back that has a correct answer.  Items and questions
share many similar properties which help to distinguish them, such as the
ones in Fig~\ref{tab:properties}.

The main question this body of work seeks to answer is this: \emph{based on
input from the student on any subset of questions in an item schedule, how
should the remaining questions be scheduled}?

\begin{table}[!p]
\label{tab:properties}
\caption{The properties of items and questions}
\vspace{12pt}
\begin{tabularx}{\textwidth}{||l|X||}
\hline \hline
\rowcolor{Gray}
Property &  Explanation  \\\hline

Difficulty & 
Proportion of students who pass the question, or who
understand the item on the first exposure.
\\ \hline
Bloom Level &
The Bloom taxonomic level of the question or the item.
An item can have an associated Bloom level; for example,
a worked-out analysis problem would qualify
the item for the analysis level.
\\ \hline
Concept & 
The concept which is being tested by the question, or
which the item is about. This could be expressions,
control structures, subprograms, and so forth.
\\ \hline
Dependencies & 
The questions or items which would need to be seen beforehand.  The dependency
information could be stored in the form of a database query; for example, an
application question on for-loops could depend on all questions whose concept
is if-statements and whose Bloom level is application.

\\ \hline \hline 
\end{tabularx}
\vspace{36pt}

\end{table}

\subsection{Novel Contributions}

The answer to this question required a series of novel contributions to the
fields of intelligent tutoring systems (ITS) and computer-aided assessment
(CAA) or computer-aided testing (CAT).  CAT and ITS systems seek to schedule
items in such a way as to maximize the efficiency of the student's learning.
The difference between a CAT and an ITS is the end goal: a CAT is used
specifically within the context of assessment, where no or almost no
non-question items are scheduled.  An ITS is used in the context of
individualized coverage of the material, using assessment as an aid.  ITSs
often contain CAT systems.  The work developed here could serve the dual
purpose of a CAT and an ITS.

The specific novel contributions are: 

\begin{itemize}

 \item The creation of a data structure in the form of a graph, whose nodes
 contain information relevant to the assessment process, and whose edges
 capture dependency relationships among questions;

 \item A modification to an existing assessment theory known as Item Response
 Theory, which however providing a mature means of assessing student ability,
 benefited from an account of dependency relationships;
 
 \item A scheduler, or algorithm whose purpose was to determine what the
 questions should be given the item parameters and the students' response
 sets;

 \item An addendum to an existing theory of memory, forgetting, and practice,
 which could then be integrated into the scheduler to provide a fuller-featured
 system.

\end{itemize}

In addition to this, the body of work rests upon other incidental novel
contributions, such as the separation of Bloom level and difficulty
\cite{castleberry2016effect}.

\section{Bloom's Taxonomy}

Bloom's cognitive taxonomy organizes questions into levels depending on the
cognitive functions required of the answerer.  The levels are: knowledge,
comprehension, application, analysis, evaluation, and synthesis.  A brief
overview is given in Table~\ref{tab:bloom}, with definitions and examples of
questions covering the concept of for-loops:

\begin{table}[!p]
\label{tab:bloom}
\caption{The levels of Bloom's taxonomy defined}
\vspace{12pt}
\begin{tabularx}{\textwidth}{||l|X|X||}
\hline \hline
\rowcolor{Gray}
Level &  Explanation & Example Question \\\hline
Knowledge & Recalling factual information.   
& {What is a for-loop?}
\\ \hline
Comprehension & Assigning meaning to information.   
& {What does the example for-loop output? (Give example.)}
\\ \hline
Application & Applying a rule to a specific instance.   
& {How can the update statement of the loop be changed to print 
only even numbers?}
\\ \hline
Analysis 
& Breaking information into parts and exploring relationships.   
& {What would happen if the update statement decremented instead of incremented the counter?}
\\ \hline
Evaluation 
& Judging the use of knowledge or the validity of an
argument.   
& {Which is better for reading user input: a for-loop or a
while-loop? Why?} 
\\ \hline
Synthesis 
& Utilizing knowledge to create a new solution to
satisfy a goal.   
& {Write a for-loop to print only even numbers up to ten.}
\\ \hline \hline 
\end{tabularx}
\vspace{36pt}
\end{table}

\begin{table}[!p]
\label{tab:grades}
\caption{Grades mapped to trait ability levels}
\vspace{12pt}
\begin{tabularx}{\textwidth}{||l|c|X||}
\hline\hline
\rowcolor{Gray}
Letter  & $\theta$  & Explanation \\\hline
F  & -3.0   & unsatisfactory \\\hline
D- & -2.5   &                \\
D  & -2.0   & minimal        \\
D+ & -1.5   &                \\\hline
C- & -1.0   &                \\
C  & -0.5   & acceptable     \\
C+ & 0.0    &                \\\hline
B- & 0.5    &                \\
B  & 1.0    & good           \\
B+ & 1.5    &                \\\hline
A- & 2.0    &                \\
A  & 2.5    & distinguished  \\
A+ & 3.0    &                \\\hline\hline
\end{tabularx}
\vspace{36pt}
\end{table}


Each category depends on the cognitive functions used in the previous category.
That is, comprehension requires knowledge, application requires comprehension,
and so forth.  Furthermore the mastery of one of the levels is with respect to
a given concept.  A student may be able to synthesize solutions to problems
dealing with expressions, but may not possess knowledge of equations, and thus
could not solve problems involving equations.

The utility of Bloom's taxonomy lies in its ability to pinpoint the underlying
cause of the student's problem-solving impasses \cite{shuhidan2011}.  
There is even evidence to suggest that Bloom levels correspond to different
electroencephalographic (EEG) signatures \cite{chatterjee2015}, implicating
distinct brain functions in their use. 

Suppose a test of mastery of loops is given with the comprehension question
``What does such-and-such loop output?'' is given, and the student reaches an
impasse.  If the question ``What are the three expressions of the loop and what
do they do?'' is asked and the student does not know, the impasse can be
attributed to a lack of knowledge about loops.  If the student does know about
the loop expressions but still cannot answer, one might instead attribute it to
a comprehension difficulty \emph{as such}; which might be remedied by giving
some examples to build intuition, then continuing to test at the comprehension
level.

Educators may have an intuitive notion of how to do this, but Bloom's taxonomy
gives the ability to examine the impact of questions scientifically.  By
identifying the tested concept and the Bloom level of exercises, one can then
form hypotheses about student responses to questions.  

\subsection{The Interpretation in Computer Science}

Bloom's taxonomy has been proven to be useful at the undergraduate level, and
particularly in the field of software engineering \cite{britto2015,
mahmood2014}.  It has seen success in program comprehension
\cite{buckley2003}, where the asking of comprehension questions fosters code
reading \cite{losada2008}. In addition, it has been useful for pinpointing the
difficulties of novice programmers in a guided learning approach
\cite{shuhidan2011}.  It been used to identify a marked preference for
higher-level problems for those able to solve them \cite{bruyn2011}
\cite{goel2004}.  At least two experiments have shown the effect of item
ordering on performance \cite{newman1988effect,castleberry2016effect}, and the
taxonomy has even been applied to create ratings of courses based on the
average Bloom levels of tasks and questions in the course
\cite{oliver2004course}.

In spite of all this, there is an ongoing debate regarding the applicability of
Bloom's taxonomy to computer science \cite{johnson2006bloom,
fuller2007developing, thompson2008bloom}.  The crux of this debate centers
around the interpretation of Bloom levels: not only how questions map to Bloom
levels, but also regarding the progression of Bloom levels over the span of a
course or curriculum.  

A tacit assumption in much of the research is that Bloom levels equate to
difficulty levels.  While there is certainly a relationship between Bloom level
and difficulty in the ``ordinary course'' of devising problems, a distinction
can be made between the two, as will be discussed in \ref{sec:reconciling}.

\subsection{Assumptions of this Work}

Prior research by the author supports the view that Bloom level and difficulty
are separable parameters of a content item \cite{castleberry2016effect}.  This
work will proceed on an assumption to that effect. 

In addition, some of the components of this body of work, particularly those
pertaining to memory and recall, are supported by many decades of empirical
research \cite{ebbinghaus}.  While the study of intelligent tutoring systems
has seen attempts to validate addendums to these theories which accommodate
re-activation of forgotten knowledge, none have undergone extensive empirical
testing as is typical for psychological models.  The components of this body of
work regarding re-activation introduce hypotheses.

\section{Item Response Theory}

Here is introduced a mature assessment theory known as Item Response Theory, an
alternative to Classical Test Theory (CCT).  Whereas Classical Test Theory
assigns a student a grade based on the student's position in a distribution of
composite test scores, Item Response Theory accounts for item difficulty, item
discrimination, the probability of guessing the question correctly.

One of the main appeals of IRT is its incorporation of item difficulty.  As
will be shown later, this is pertinent to the interpretation of Bloom levels.  

According to Item Response Theory (IRT), in what is known as the 3PL or 3
parameter logistic model, the probability $p_i$ that a student answers
correctly the $i^{th}$ question on a test, is given by:

\begin{equations}
 \label{eq:irt}
  p_i(\theta_s) = \gamma_i + \frac{1-\gamma_i}{1+e^{\alpha_i(\theta-\beta_i)}}
\end{equations}

where:

\begin{itemize} 

 \item $\alpha$ is the item discrimination, or how well the item can
 distinguish students of varying trait ability;

 \item $\beta$ is the question difficulty, 

 \item $\gamma$ is the probability of guessing the answer correctly,

 \item and $\theta$ is the \emph{trait ability} of the student, or the
 student's particular ability to answer that question correctly.

\end{itemize} 

A graph of the IRT curve for the parameters $\alpha=1, \beta=0, \gamma=0$ is
shown \ref{fig:irt}.

\begin{figure}[p!]
 \label{fig:irt}
 \includegraphics{fig/irt.eps} 
 \caption{A probability curve in Item Response Theory.}
\end{figure}

\begin{figure}[p!]
 \label{fig:mle}
 \includegraphics{fig/mle.eps} 
 \caption{A maximum-likelihood estimation for IRT}
\end{figure}

Note that $\alpha_i$, $\beta_i$, and $\gamma_i$ are parameters of the item $i$;
however $\theta_s$ is particular to the student $s$.  

The question difficulty $\beta$ can be estimated by the proportion of students
who pass the question.  The value $\beta$ is analogous to standard deviation in
classical test theory.  If a model of student ability based upon the number of
standard deviations from the mean is desired, and the scores are normally
distributed, then the $\beta$ values can simply be obtained from the
probability density function for a Student's t distribution.

Supposing $\phi$ is the proportion of students who pass the question, $f(v, x)$
is the probability density function for the Student's t-distribution with $v$
degrees of freedom then the relationship between $\phi$ and $\beta$ in such an
assignment would be given by

\begin{equation}
  \phi = \int_{\beta}^{\infty} f(v, x) \ dx
\end{equation}

That is, $\beta$ is the number of standard deviations such that the area from
$\beta$ to $\infty$ under the curve is equal to the proportion of students who
pass the question.  The value can also be obtained from a t-table.

Insofar as the intelligent tutoring system is concerned, the instructor or user
is free to assign $\beta$.  Varying assignments of the $\beta$ scores will
slightly alter the behavior of the system; if a given question is tagged as
easier (that is, assigned a lower $\beta$ value), it will have a greater chance
of appearing earlier in any given student's schedule.

If only $\beta$ is used as a parameter, the resulting model is known as the
1PL or 1 paramter logistic model:

\begin{equations}
 \label{eq:1pl}
  p_i(\theta_s) = \frac{1}{1+e^{\theta-\beta_i}}
\end{equations}

\begin{figure}[p!]
 \label{fig:ctt}
 \includegraphics{fig/ctt.eps} 
 \caption{The standard normal curve; the distribution of test scores
  as modeled by Classical Test Theory}
\end{figure}

\begin{figure}[p!]
 \label{fig:pl1}
 \includegraphics{fig/pl1.eps} 
 \caption{The 1PL Item Response Theory model, where $\beta$=1}
\end{figure}


The item discrimination, $\alpha_i$, is defined as the Pearson product-moment
correlation coefficient between the item responses and the composite score on
the test.  Since the item responses are dichotomous, a correlation known as the
point-biserial correlation is computed.  It is defined as:

\begin{equations}
  \alpha_i = \frac{M_1 - M_0}{s_n} \sqrt{\frac{n_0 n_1}{n^2}}
\end{equations}

where $M_1$ is the mean composite score for all correct responses; $M_0$ is the
mean composite score for incorrect responses; $n_0$, $n_1$, and $n$ are number
of incorrect, correct, and total responses, respectively; and the standard
deviation $s_n$ is defined as:

\begin{equations}
  s_n = \sqrt{ \frac{1}{n} \displaystyle\sum_{i=1}^n (X_i - \overline{X})^2} 
\end{equations}

where $X_i$ is the ith response and $\overline{X}$ is the mean of the
responses.  $\alpha$ values have range $[-1, 1]$.  Positive values indicate
that the question is positively discriminating; it is a question which is
useful for gauging the constructs which the composite score seeks to measure.
Students who succeed on a question with a higher $\alpha$ are more likely to
have higher composite scores.  Likewise, negative values indicate that the
question is inversely related to composite score.  Values near zero indicate
that the question is unrelated.

If only $\alpha$ and $\beta$ are used as parameters, the resulting model is
known as the 2PL or 2 paramter logistic model:

\begin{equations}
 \label{eq:2pl}
  p_i(\theta_s) = \frac{1}{1+e^{\alpha(\theta-\beta_i)}}
\end{equations}


The probability of guessing is defined as

\begin{equations}
  \gamma_i = \frac{1}{m_i}
\end{equations}

where $m_i$ is the number of options in a multiple choice question.  It is
assumed that each option is equally likely to be chosen if a student with
very low trait ability were to answer the question; that is, there can be
no allowance for logical process-of-elimination tactics.

Addition of the probability of guessing results in the 3PL model, given
in Eq~\ref{eq:irt}.

If the trait ability of the student is known in addition to the item
parameters, then the probability of a correct response can be calculated.
However, it is more often than not the case that the response set of the
student is known, and trait ability is unknown.  In such a case, a variety of
techniques are deployable.

\begin{figure}[p!]
 \label{fig:pl2}
 \includegraphics{fig/pl2.eps} 
 \caption{The 2PL Item Response Theory model, where the item discrimination 
 $\alpha=.5$ and $\beta=0$}
\end{figure}

\begin{figure}[p!]
 \label{fig:pl3}
 \includegraphics{fig/pl3.eps} 
 \caption{The PL3 Item Response Theory model, where $\alpha=1$ and $\beta=0$,
 and probability of guessing $\gamma=.25$}
\end{figure}

Trait ability in IRT can be obtained using a maximum likelihood estimation
(MLE) method, which finds a maximum-likelihood estimate of $\theta$ by testing
a range of $\theta$ values with the IRT formula \cite{baker2004}.  Later it
will be shown how $\theta$ can be estimated.  Another potential technique is
Newton-Raphson, which in most other applications would be the more efficient
and desirable method \cite{hambleton1991fundamentals}. 

%The utility of IRT is that it requires fewer questions to gauge trait ability
%due to its account of other factors ($\alpha, \beta, \gamma$). It is thus
%reputed to be a more mature means of assessment than CTT.

Until now, the fusion of IRT and Bloom's taxonomy in conjunction with a testing
algorithm or scheduling algorithm has existed in the literature as a
possibility \cite{sitthisak} \cite{ghulman2009modern}
\cite{lilley2005generation} \cite{osborne2013grounded}. This work seeks to
reconcile the two by offering a compatible interpretation of Bloom's taxonomy
which works in condordance with IRT.

\subsection{Evaluation of Trait Ability}

Consider a set of content items or questions for which the answer is either
incorrect or correct.  This is true in the case of multiple choice questions
(as well as true-false, which is a subset of multiple-choice questions).

Suppose the student has a response set for content items: 

\begin{equations}
  \label{eq:responseset}
  X_s = x_{s1}, x_{s2}, \ldots, x_{si}, \ldots x_{sn}
\end{equations}

Here $X_s$ is the response vector of student $s$, and $x_{si}$ is the
correctness of the response to content item $i$ by student $s$; it is zero if
the response is incorrect, or 1 if it is correct.  

These questions may be of various and sundry discriminations, difficulties, and
probabilities of guessing.  The first question may have $\alpha_1=.5$,
$\beta_1=1$, and $\gamma_1=.25$; the second may only differ in its difficulty,
for example $\beta_2=-1$.  However, it shall be assumed that the set of
content items for which the student has provided responses are for the same
concept and the same Bloom taxonomic level.  The reason for this assumption
will become apparent later.

Recall that the probability $p_{si}$ of student $s$ answering the $i^{th}$
question correctly is:

\begin{equations}
  p_{si}(\theta_s) = \gamma_i + \frac{1-\gamma_i}{1+e^{\alpha_i(\theta_s-\beta_i)}}
  \tag{\ref{eq:irt}}
\end{equations}

Since $\theta_s$ is unknown but the response set is known, one method for
determining $\theta_s$ is by guessing a range of possible values.  First, one
can define a function $f_{si}$:

\begin{equations}
f_{si}(\theta_s) =\left\{
         \begin{array}{ll}
               p_{si}(\theta_s) & \mathrm{if}\  x_{si} = 1 \\
               q_{si}(\theta_s) & \mathrm{otherwise}
         \end{array}
       \right.
\end{equations}
% TODO: (SRB) Do you mean p_{si} above when you type p_i?

where 

\begin{equations}
   q_{si}(\theta_s)  = 1 - p_{si}(\theta_s).
\end{equations}

That is, $f_{si}$ assumes the probability $p_{si}$ if answered correctly and
$q_{si}$ if not answered correctly.  Proceeding on the assumption that each of
the observations (that is, responses in the response set) is independent, the
probablity of observing a total response set given a particular $\theta_s$
value is the product of the probabilities $f_{si}$ for all $i$, $1 \leq i \leq
n$, or

\begin{equations}
  \prod_{i=1}^n f_{si}(\theta_s).
\end{equations}

Supposing that there exists some $\theta_s$ which maximizes this product,
the most likely value for the student's true trait ability $\theta_s$ is
defined by:

\begin{equations}
  \theta_s = 
  \underset{\theta}{\textrm{argmax}}
  \Bigg[ 
  \prod_{i=1}^n f_{si}(\theta).
  \Bigg]
\end{equations}


that is, that value of $\theta_s$ which maximizes the product which gives the
probability of all the observations occuring together, given $\theta$.  To
obtain this, products for a range of possible $\theta$ values are calculated.

In the intelligent tutoring system which has been constructed, there are only
thirteen such values, drawing from the +/- system.  The mapping of grades to
trait ability levels is given in Table~\ref{tab:grades}.  This mapping could
also be applied to difficulty levels of questions.  A ``C question'' is one
which students of average trait ability (and perhaps just more than half the
class) may be expected to answer; an ``A+ question'' is a very difficult
question which students of only A+ ability at the time of asking may be
expected to answer; and an ``F question'' is one which may be used to determine
if a student's trait ability is minimally satisfactory.  This mapping lends to
a intuitive understanding of trait ability--as the familiar letter grade.  An
MLE may thereby effectively grade the student.

% TODO: (SRB) Something should be said about the confidence in the trait
% ability assessment given above. In principle, a letter grade could be
% obtained from 2 questions.

To that end, rather than using a fine-grained MLE, it makes practical sense to
calculate products for these thirteen values of $\theta$, since higher
granularity than the +/- system is not useful for final grade assignment, nor
is necessarily more intuitive for the student or instructor.  Restricting the
products calculated to this set of values also makes sense from an efficiency
standpoint.  The total time complexity of the MLE is linear in the size of the
response set, regardless of the number of guesses in $\theta$; while not
asymptotically reduced from using a smaller step size, it is five times faster
than the typical choice of $10^{-1}$, and fifty times faster than the
fine-grained $10^{-2}$.  A graph calculating likelihoods for MLE is depicted in
Figure~\ref{fig:mle}.

% Talk about gradient descent and Newton-Raphson

The total number of MLE estimations for trait ability throughout a course is
equal to the total number of responses for all questions throughout a course,
which can be quite large; it is proportional to the number of students times
the number of items. 


\section{Previous Literature}
\label{sec:litreview}

\subsection{Computer-Aided Testing}

There are three main advantages of CATs.  The first is that they easily lend
themselves to formative testing; the second is that they are amenable to the
scientific exploration of learning theories, since the response data becomes
immediately available (and testing algorithms are easier to implement on a
computer-based medium); and the third is that they allow for the implementation
of algorithms to determine the series of questions to be asked, allowing for
dynamic testing.

Any form of test that provides feedback is known as formative testing or
formative assessment \cite{bull2003blueprint}.  Empirical studies have
demonstrated the equivalence of paper-based and computer-based testing when a
formative assessment component is absent from computer-based testing
\cite{mead1993equivalence}, however when computer-based methods do provide
formative assessment the effect sizes in overall performance tend to be large
\cite{bangert1991instructional} \cite{lawton2012online}.

A clear advantage of CATs is that they lend themselves to greater number of
testable hypotheses than paper-based methods, since the effect of an
intervening algorithm can be examined.  \cite{mayer2002multimedia} The notion
of ``testing algorithms'' has existed almost as long as CATs
\cite{wainer2000computerized}, and mainly concerns those algorithms which
partition the test into decision points around the students responses, then
select the next question based upon the prior response set.

A theme in the workflow of CATs has been identified; namely that almost all
cats begin with an initiation of ability estimate, then proceed with selection
of an item and the administration of that item, followed by an update to
ability estimate, and finally a check on the stopping criterion
\cite{veldkamp2013bayesian}.  Some CATs use a technique known as shadow
testing, which is used to set initial item parameters such as difficulty and
determine the validity of items, so that the pool of questions may be pruned to
a useful subset \cite{van2000computerized}.

Not all CATs are concerned with selecting the next question; there is a
categorization of methods used to devise an optimal test, including
heuristic-based test assembly, which selects a new question given prior
response data, however some methods seek the whole test at once
\cite{van1998optimal}.  Some work has been done on the estimation of concept
relationship degree for the purpose of pruning questions, as well as developing
a personalized training pathway based on a genetic algorithm
\cite{chen2008intelligent}.  

The potential for CATs to be used outside the scope of a single course have
been explored, particularly in curriculum sequencing (which is concerned with
the order in which a student takes courses).  A CAT has been used to give
preliminary tests for courses (what the authors call ``courseware''), and
recommendations for courses are given in an order corresponding to the student
trait ability estimates \cite{chen2006personalized}.


%CATs are widespread enough that as of 2011, a guide was written with the intent
%of evaluating a CAT \cite{davey2011guide}, which addresses several desirable
%features of a CAT.
% TODO: show that we meet these.

\subsection{Computer-Aided Testing and Item Response Theory}

Only a few mature CATs exist, but IRT and its relatives (the 1PL and 2PL
models) are not an uncommon choice of ability estimation for those that do.
One CAT in particular uses the Rasch model, or the 1PL model (accounting
exclusively for difficulty) \cite{chen2005personalized}, and seeks a ``guided
path'' for learners, namely a sequence of questions to ask.

Another CAT, which contains features which place it within the scope of ITSs,
uses IRT to estimate English language-learners' ability, then recommend
articles for them to read based upon their trait ability estimation
\cite{yarandi2011personalised}.  It is noteworthy that the articles which the
students are referred to are also tagged with difficulty levels,  which is a
feature in common with the non-question items of the present work.

One CAT in particular uses IRT with what is called a process-based model of
learning, in which individual processes are assigned trait ability estimates
\cite{tatsuoka1990toward}.  The notion behind such an endeavor is that since
problem-solving itself follows a process-based approach, the most effective way
to model student performance is to obtain trait ability estimates for every
step of the problem-solving process.  In this way, it is possible to identify
impasses within a problem.  

\subsection{Computer-Aided Testing and Bloom's Taxonomy}

It has been argued that outside of computer-based systems, one of the
requirements for computer science problems should be the use of a taxonomic
system for educational objectives, to identify how the question fits into the
progression of learning across a concept \cite{loria-saenz2008}.  The primary
motivation for using Bloom's taxonomy in a CAT is to create a finer-grained
competency model that can be used to personalize assessment
\cite{sitthisak2007towards}, namely by identifying impasses in such a
progression. 

Specialized CATs which incorporate Bloom's taxonomy typically treat the levels
of Bloom's taxonomy as difficulty levels. A few CATs separate the levels and
develop ability estimates for each.  One in particular assumes the form of a
matrix, with two dimensions being levels of Bloom's taxonomy, the third being
specific skills \cite{raykova2011adaptive}.

Specialized CATs appear particularly useful in aiding students to reach the
effective to reach comprehension and application levels of cognitive domain
\cite{ccepni2006effects}.  Some studies have even shown improvements in the
higher levels of Bloom's taxonomy, especially evaluation
\cite{hopson2001using}.  Targetting the evaluation and synthesis levels has
proven to be challenging for CATs which rely on Bloom's taxonomy, but efforts
in other fields, such as anatomy and physiology, have shown that
multiple-choice questions (MCQs) can be adapted effectively to the higher
levels \cite{de2011computer}.  For example, in the case of the anatomy and
physiology-based CAT, the evaluation level asks for a diagnosis of a patient
issue given symptoms.  It is agreed by the experts who evaluated the Bloom
levels of the questions that there was a matter of correctness about subsets of
questions at the evaluation and synthesis levels, as is required for the
closed-ended form of MCQs \cite{lemons2013questions}.  At least one other CAT
developed for use in the biological sciences has demonstrated that MCQs can be
adapted to the higher Bloom levels \cite{hernan2008testing},
\cite{lemons2013questions}.  General techniques have been outlined for doing so
\cite{duke2001using}, and various formats of MCQs have been evaluated in the
context of Bloom's taxonomy in terms of their effectiveness on learning
\cite{haladyna1992effectiveness}.

It is argued that innovative techniques may be used to test the higher levels
in mathematics and computer science education, where traditionally
evaluation-and synthesis-level questions have required a demonstration of tight
reasoning \cite{sim2004implementation}.  Success to adapt MCQs up to the
evaluation level has also been achieved in computer science
\cite{castleberry2016effect}.  One strategy for testing at the analysis level
is to present the student with a set of short logical arguments, then ask the
student to identify an invalid argument; or to identify a valid argument among
invalid arguments.   To solve such a problem, the student must be able to
reason about the logical implications.  A strategy for testing the evaluation
level is to present the student with statements which use vocabulary
appropriate to the topic but which are nonsensical; the student must select the
only sensical statement from the set.  To solve such a problem, the student
must be able to distinguish shades of meaning of the words, Another strategy is
the ``select the best response'' format, where the student must evaluate the
strength of logical reasoning for a proposition; or questions which ask the
student to identify a sound proof of a proposition from a set of unsound
proofs.

%It is generally agreed that MCQs possess limitations. One study proposes the
%development of questions which enhance learner self-regulation, which includes
%forethought (studying) and reflection on the results
%\cite{nicol2007assessment}.

\subsection{Works Mentioning Bloom's Taxonomy and Item Response Theory}

There has been one suggestion by Sitthisak et al.  linking Item Response Theory
with Bloom's taxonomy which interprets Bloom levels as difficulty levels.  That
is, the values of $\beta$ from Item Response Theory are mapped directly to
Bloom levels \cite{sitthisak}.  This suggestion exists in the form of a
proposal to implement a computer-aided tester using Item Response Theory.

Another study identifies difficulty by levels of Bloom's taxonomic levels, and
evidently applies Item Response Theory to scoring of the same questions;
however it is unclear how the Bloom levels are mapped to Item Response Theory
$\beta$ values, or whether or not the mentioned difficulties were hypothesized
or actual difficulties \cite{osborne2013grounded}.  

A CAT was developed which sorts questions based on topic and Bloom level.
Questions were tagged using expert evaluations on Bloom levels, and difficulty
level were assigned to the questions based upon Bloom levels.  This CAT would
thus build a competency profile on the student.  However, no scheduler was
present in the CAT \cite{lilley2005generation}.

It is discussed in Sec~\ref{sec:reconciling} why it is desirable for the
current intelligent tutoring system to maintain difficulty and Bloom levels as
separate entities.

Ghulman et al. may be the closest to a full separation of Bloom's taxonomy and
difficulty, as well as establishing the connection between Bloom's taxonomy and
Item Response Theory, as it categorizes questions by Bloom level before
applying a Rasch model to score them \cite{ghulman2009modern}.  The Rasch model
is a model simpler in nature than Item Response Theory which uses one parameter
and is derived from the data set itself.

\subsection{Categories of Systems}

Conole and Warburton have developed a classification system for CATs.
Web-based systems \cite{wang2004web} fall under networked systems, which in
turn fall under computer-based assessment utilities, which in turn falls under
computer-assisted assessment \cite{conole2005review}.  Conole and Warbuton
identify that Bloom's taxonomy is often used as an outcomes-based
categorization tool for questions, and that Classical Test Theory and Item
Response Theory are the two main ways in which computer-aided systems score
questions; however these are mentioned in isolation.

Bejar distinguishes between two types of systems: deficit assessment and error
analysis \cite{bejar1984educational}.  Deficit assessment is concerned with
finding where a student is lacking (such as in trait ability), and error
analysis with what steps in the problem-solving process the student is making
errors in.  This distinction is similar to Anderson's characterization-based
models versus process-based models in intelligent tutoring systems; where
characterization-based models attempt to create a representation of the
student's strengths, process-based models attempt to simulate the student's
problem-solving processes \cite{anderson1996act}.  The current work would fall
squarely within the realm of Bejar's deficit assessment-type system, or
Anderson's characterization-based model.

\subsection{Memory and Forgetting}

Ebbinghaus is credited with having created one of the first mathematical
memories of rememberance and forgetting \cite{ebbinghaus}.  In particular, he
identified that recall drops off exponentially with time.  Empirical studies
since then have supported his original finding.

John Anderson developed a model for process-based learning which could provide
the foundation for an intelligent tutoring system \cite{anderson1996act}.  He
called this Adaptive Control of Thought-Rational (ACT-R).  In ACT-R, there are
goals, akin to problem statements; and rules, or processes used to solve
problems; and finally facts, or knowledge utilized in the course of applying
rules.  In this regard, the structure of an ACT-R model resembles a logic
program.  Anderson also developed a memory model for ACT-R fact and rule recall
loosely based on Ebbinghaus' recall functions, as well as models of
re-activation of knowledge generalized from his own empirial research
\cite{bacon2003assessing}.  Anderson's extension of ACT-R to the scope of
visual learning and response times has been successful in producing models
which accurately predicting the time it takes to navigate menus in a web
interface \cite{anderson1997act}, demonstrating the power of the ACT-R theory.

\subsection{Executability} 

Finally, some supplementary work on the executability component of the
intelligent tutoring system has been done \cite{castleberry2011}.  One
component of the intelligent tutoring system allows for arbitrary code to
execute before a question or item is displayed to the student.  This feature
was inspired by earlier work on what are known as executable papers.
Executable papers are academic publications residing inside of a virtual (or
other) machine which have editable codes on the front-end and compilers or
interpreters on the backend.  The front-end codes may be edited, then the
paper may be re-compiled so that results from the codes are substituted back
into the paper.  

This idea was originally intended for use with computational science codes, but
was adapted for statistics codes \cite{castleberry2013}.  It has been
integrated into the present work to allow for sophisticated random-number
generation, and for the nature of the questions and items to change in response
to the student or other contextual data.


\subsection{Reconciling Bloom's Taxonomy with Item Response Theory}
\label{sec:reconciling}

A popular interpretation of Bloom's cognitive levels is that they are
difficulty levels, and that these difficulty levels are fixed
\cite{newman1988effect,oliver2004course,lord2007moving,
johnson2006bloom,fuller2007developing}.  Knowledge is easy; synthesis is hard.
We will call this the Bloom-equals-difficulty hypothesis.

A creative interpretation of this hypothesis is that per-student difficulty can
be explained in terms of the demotion of Bloom levels.  The synthesis \emph{and
therefore difficult} questions, once the solution is obtained, are reduced to
knowledge \emph{and therefore easy} questions.  Certainly, this is one possible
scenario.  However, it does not explain a student's ability to answer
altogether distinct synthesis problems; this is to suggest students engage the
cognitive functions of synthesis, presumably as an effect of learning the use
of those functions.  To maintain that Bloom equals difficulty would either
require sustaining the view that students are able to pass the questions
because of a demotion of the cognitive functions required (which poses a rather
cynical view of cognition and of education); or else some notion of trait
ability--in particular one which offsets difficulty, allowing an increase in
the probability of passing the question.

The latter option takes the shape of a primeval form of Item Response Theory,
mapping Bloom levels to $\beta$.  This is highly convenient for the hypothesis
that Bloom equals difficulty, as it becomes agreeable to evaluation with Item
Response Theory.  Nontheless, there are still several issues with the
hypothesis.

Consider first that exposure to knowledge and knowledge questions causes an
increase in $\theta$, provided the student acquires the knowledge.  According
to Bloom's theory, this would increase the probability of answering
comprehension questions correctly.  This agrees with the theory.  However,
according to Item Response Theory, it would also increase the probability of
answering application questions correctly even if no comprehension-level
questions are asked.  In fact, it is theoretically possible to design tests
with sufficient numbers of knowledge questions to place, for any $\beta$,
$\theta-\beta$ much greater than zero--that is for example to say, asking a
sufficient number of knowledge questions should give the reasonable assurance
that application-level ability is high enough not to bother testing it.

As many educators will no doubt agree, this is not the case. Even if a student
scores exceptionally well on knowledge questions, it gives no such assurance
that the student will score similarly on application-level questions.  Even if
knowledge and application scores are found to be correlated, there may be an
additional underlying factor contributing to both scores (such as
intelligence).  The application level is \emph{qualitatively} different; hence
it needs to be tested in spite of the value of $\theta$.

Second, if the hypotheses is strictly true, then there can exist no
counterexample to the claim.  That is to say that there exists no problem on a
lower Bloom level which is more difficult than a problem on a higher Bloom
level.  To take an example, it would be mistaken to call an application problem
easier than any comprehension problem. 

Counterexmaples with intuitive appeal can be constructed.  Take for-loops for
example.  To ask a student to print out the result of a simple loop which
prints numbers 1 to 10 should present an easy enough task.  This is an
application of the many rules of for-loops, but which does not require creative
synthesis or critical thinking on the part of the student (hence it is an
application problem). 

Consider then asking the student to impose a flowchart over the same code,
including the initialization, condition and update statements and the body of
the loop, and to indicate the start and stop of the loop.  Suppose the student
has seen flowcharts over similar loops before.  This is a test of
comprehension--in particular, comprehension of the internal workings (the
control flow) of the loop.    

The application problem above requires only an intuitive comprehension of the
loop: ``\emph{It prints the sequence from 1 up to 10 in steps of 1}'', the
student may realize.  It demands only shallow comprehension and the rule to be
applied is simple.  In the comprehension problem, on the other hand, a higher
degree of precision in comprehension of the loop is demanded to solve the
problem.  The comprehension problem also depends on mastery of the knowledge,
comprehension, and application of flowchart symbols.  Regardless, explaining
the control flow of the loop in full detail is a more difficult undertaking
than simply printing its output. 

Likewise, it is possible to ask the student to synthesize a loop printing the
first ten powers of two, then ask a comparatively difficult analysis question
in the form of an obfuscated loop code.  Impasses in this situation may be
attributed to a lack of knowledge or comprehension of the constructs used in
the presented code.  In the synthesis problem, the student has the advantage of
using known syntax; it is a ``constructed-response'' type of question, rather
than a closed-form question \cite{kuechler2010performance}. 

Third, by Bloom's own admission, it is possible to skip levels for certain
concepts \cite{bloom1956}.  In this case, the ordinality of Bloom levels
breaks down.  A plausible example is given above.  It is possible to teach how
to obtain the numeric sequences printed by for-loops before covering in full
detail the control flow of the loop.  In that particular case, application of
the rule does not depend on comprehension of the code construct.

Alternative interpretations of Bloom levels distinguish between facility or
difficulty and complexity of a problem \cite{hill1981testing,
thompson2008bloom}.  In such interpreations, the cognitive complexity has an
orthogonal relationship to the item difficulty.  The problem with such
interpretations, however, is that they do not readily explain the moderate
correlation that does exist between mean performance and Bloom level
\cite{hill1981testing}.


