
\section{Introduction}

\subsection{Problem Definition}

A \emph{content item} is some unit of information, such as a statement, graph,
table, example, and so forth.  An educationl television program, interactive
tutorial or university lecture (generally, these may all be called programs)
can be broken down into these units of information.  Call an item $\chi$; then
the $i^{th}$ item to be seen is $\chi_i$, and the time at which it is seen
$t_i$.  Then $(\chi_i, t_i)$ represents the $i^{th}$ item and its scheduled
time.  

For that matter, any program can be thought of as a sequence of these tuples,
thereby forming a schedule of items: 

\begin{equations}
\label{eq:schedule}
   X = \langle (\chi_1, t_1), (\chi_2, t_2), \ldots (\chi_n, t_n) \rangle.
\end{equations}
\vspace{2pt}

Furthermore $\chi$ could be a question, which could be thought of as an item
which accepts a response back that has a correct answer.  Items and questions
share many similar properties which help to distinguish them, such as the
ones in Fig~\ref{fig:properties}.

The main question this body of work seeks to answer is this: \emph{based on
input from the student on any subset of questions in an item schedule, how
should the remaining questions be scheduled}?

\subsection{Novel Contributions}

The answer to this question required a series of novel contributions to the
fields of intelligent tutoring systems (ITS) and computer-aided assessment
(CAT).  Specifically:

\begin{itemize}

 \item The creation of a data structure in the form of a graph, whose nodes
 contain information relevant to the assessment process, and whose edges
 capture dependency relationships among questions;

 \item A modification to an existing assessment theory known as Item Response
 Theory, which however providing a mature means of assessing student ability,
 benefited from an account of dependency relationships;
 
 \item A scheduler, or algorithm whose purpose was to determine what the
 questions should be given the item parameters and the students' response
 sets;

 \item An addendum to an existing theory of memory, forgetting, and practice,
 which could then be integrated into the scheduler to provide a fuller-featured
 system.

\end{itemize}

In addition to this, the body of work rests upon other incidental novel
contributions, such as the separation of Bloom level and difficulty
\cite{castleberry}.

\section{Bloom's Taxonomy}

Bloom's cognitive taxonomy organizes questions into levels depending on the
cognitive functions required of the answerer.  The levels are: knowledge,
comprehension, application, analysis, evaluation, and synthesis.  A brief
overview is given in Table~\ref{tab:bloom}, with definitions and examples of
questions covering the concept of for-loops:

\begin{table}[!p]
\label{tab:bloom}
\caption{The levels of Bloom's taxonomy defined}
\vspace{12pt}
\begin{tabularx}{\textwidth}{||l|X|X||}
\hline \hline
\rowcolor{Gray}
Level &  Explanation & Example Question \\\hline
Knowledge & Recalling factual information.   
& {What is a for-loop?}
\\ \hline
Comprehension & Assigning meaning to information.   
& {What does the example for-loop output? (Give example.)}
\\ \hline
Application & Applying a rule to a specific instance.   
& {How can the update statement of the loop be changed to print 
only even numbers?}
\\ \hline
Analysis 
& Breaking information into parts and exploring relationships.   
& {What would happen if the update statement decremented instead of incremented the counter?}
\\ \hline
Evaluation 
& Judging the use of knowledge or the validity of an
argument.   
& {Which is better for reading user input: a for-loop or a
while-loop? Why?} 
\\ \hline
Synthesis 
& Utilizing knowledge to create a new solution to
satisfy a goal.   
& {Write a for-loop to print only even numbers up to ten.}
\\ \hline \hline 
\end{tabularx}
\vspace{36pt}
\end{table}

\begin{table}[!p]
\label{tab:grades}
\caption{Grades mapped to trait ability levels}
\vspace{12pt}
\begin{tabularx}{\textwidth}{||l|c|X||}
\hline\hline
\rowcolor{Gray}
Letter  & $\theta$  & Explanation \\\hline
F  & -3.0   & unsatisfactory \\\hline
D- & -2.5   &                \\
D  & -2.0   & minimal        \\
D+ & -1.5   &                \\\hline
C- & -1.0   &                \\
C  & -0.5   & acceptable     \\
C+ & 0.0    &                \\\hline
B- & 0.5    &                \\
B  & 1.0    & good           \\
B+ & 1.5    &                \\\hline
A- & 2.0    &                \\
A  & 2.5    & distinguished  \\
A+ & 3.0    &                \\\hline\hline
\end{tabularx}
\vspace{36pt}
\end{table}


Each category depends on the cognitive functions used in the previous category.
That is, comprehension requires knowledge, application requires comprehension,
and so forth.  Furthermore the mastery of one of the levels is with respect to
a given concept.  A student may be able to synthesize solutions to problems
dealing with expressions, but may not possess knowledge of equations, and thus
could not solve problems involving equations.

The utility of Bloom's taxonomy lies in its ability to pinpoint the underlying
cause of the student's problem-solving impasses \cite{shuhidan2011}.  Suppose a
test of mastery of loops is given with the comprehension question ``What does
such-and-such loop output?'' is given, and the student reaches an impasse.  If
the question ``What are the three expressions of the loop and what do they
do?'' is asked and the student does not know, the impasse can be attributed to
a lack of knowledge about loops.  If the student does know about the loop
expressions but still cannot answer, one might instead attribute it to a
comprehension difficulty \emph{as such}; which might be remedied by giving some
examples to build intuition, then continuing to test at the comprehension
level.

Educators may have an intuitive notion of how to do this, but Bloom's taxonomy
gives the ability to examine the impact of questions scientifically.  By
identifying the tested concept and the Bloom level of exercises, one can then
form hypotheses about student responses to questions.  

\subsection{The Interpretation in Computer Science}

Bloom's taxonomy has been proven to be useful at the undergraduate level, and
particularly in the field of software engineering \cite{britto2015,
mahmood2014}.  It has seen success in program comprehension
\cite{buckley2003}, where the asking of comprehension questions fosters code
reading \cite{losada2008}. In addition, it has been useful for pinpointing the
difficulties of novice programmers in a guided learning approach
\cite{shuhidan2011}.  It been used to identify a marked preference for
higher-level problems for those able to solve them \cite{bruyn2011}
\cite{goel2004}.  At least two experiments have shown the effect of item
ordering on performance \cite{newman1988effect,castleberry2016effect}, and the
taxonomy has even been applied to create ratings of courses based on the
average Bloom levels of tasks and questions in the course
\cite{oliver2004course}.

In spite of all this, there is an ongoing debate regarding the applicability of
Bloom's taxonomy to computer science \cite{johnson2006bloom,
fuller2007developing, thompson2008bloom}.  The crux of this debate centers
around the interpretation of Bloom levels: not only how questions map to Bloom
levels, but also regarding the progression of Bloom levels over the span of a
course or curriculum.  

A tacit assumption in much of the research is that Bloom levels equate to
difficulty levels.  While there is certainly a relationship between Bloom level
and difficulty in the ``ordinary course'' of devising problems, a distinction
can be made between the two.
% TODO: (SRB) cite the papers you cite in the difficulty paper regarding this
% assumption.

\subsection{Assumptions of this Work}

% TODO: (SRB) cite the prior research. I know it's not published yet.
Prior research by the author supports the view that Bloom level and difficulty
are separable parameters of a content item.  This work will proceed on the
assumption to that effect. 

In addition, some of the components of this body of work, particularly those
pertaining to memory and recall, are supported by many decades of empirical
research \cite{ebbinghaus}.  While the study of intelligent tutoring systems
has seen attempts to validate addendums to these theories which accommodate
re-activation of forgotten knowledge, none have undergone extensive empirical
testing as is typical for psychological models.  The components of this body of
work regarding re-activation introduce hypotheses.

% TODO: more on this

\section{Item Response Theory}

Here is introduced a mature assessment theory known as Item Response Theory, an
alternative to Classical Test Theory (CCT).  Whereas Classical Test Theory
assigns a student a grade based on the student's position in a distribution of
composite test scores, Item Response Theory accounts for item difficulty, item
discrimination, the probability of guessing the question correctly.

One of the main appeals of IRT is its incorporation of item difficulty.  As
will be shown later, this is pertinent to the interpretation of Bloom levels.  

According to Item Response Theory (IRT), the probability $p_i$ that a student
answers correctly the $i^{th}$ question on a test, is given by:

\begin{equations}
 \label{eq:irt}
  p_i(\theta_s) = \gamma_i + \frac{1-\gamma_i}{1+e^{\alpha_i(\theta-\beta_i)}}
\end{equations}

where:

\begin{itemize} 

 \item $\alpha$ is the item discrimination, or how well the item can
 distinguish students of varying trait ability;

 \item $\beta$ is the question difficulty, 

 \item $\gamma$ is the probability of guessing the answer correctly,

 \item and $\theta$ is the \emph{trait ability} of the student, or the
 student's particular ability to answer that question correctly.

\end{itemize} 

A graph of the IRT curve for the parameters $\alpha=1, \beta=0, \gamma=0$ is
shown \ref{fig:irt}.

\begin{figure}[p!]
 \label{fig:irt}
 \includegraphics{fig/irt.eps} 
 \caption{A probability curve in Item Response Theory.}
\end{figure}

\begin{figure}[p!]
 \label{fig:mle}
 \includegraphics{fig/mle.eps} 
 \caption{A maximum-likelihood estimation for IRT}
\end{figure}

Note that $\alpha_i$, $\beta_i$, and $\gamma_i$ are parameters of the item $i$;
however $\theta_s$ is particular to the student $s$.  

The question difficulty $\beta$ can be estimated by the proportion of students
who pass the question.  The value $\beta$ is analogous to standard deviation in
classical test theory.  If a model of student ability based upon the number of
standard deviations from the mean is desired, and the scores are normally
distributed, then the $\beta$ values can simply be obtained from the
probability density function for a Student's t distribution.

Supposing $\phi$ is the proportion of students who pass the question, $f(v, x)$
is the probability density function for the Student's t-distribution with $v$
degrees of freedom then the relationship between $\phi$ and $\beta$ in such an
assignment would be given by

%\begin{equation}
%  p(x) = \frac{e^{-\frac{1}{2} x^2}}{\sqrt{2\pi}}
%\end{equation}


\begin{equation}
  \phi = \int_{\beta}^{\infty} f(v, x) \ dx
\end{equation}

That is, $\beta$ is the number of standard deviations such that the area from
$\beta$ to $\infty$ under the curve is equal to the proportion of students who
pass the question.  The value can also be obtained from a t-table.

Insofar as the intelligent tutoring system is concerned, the instructor or user
is free to assign $\beta$.  Varying assignments of the $\beta$ scores will
slightly alter the behavior of the system; if a given question is tagged as
easier (that is, assigned a lower $\beta$ value), it will have a greater chance
of appearing earlier in any given student's schedule.

The item discrimination, $\alpha_i$, is defined as the Pearson product-moment
correlation coefficient between the item responses and the composite score on
the test.  Since the item responses are dichotomous, a correlation known as the
point-biserial correlation is computed.  It is defined as:

\begin{equations}
  \alpha_i = \frac{M_1 - M_0}{s_n} \sqrt{\frac{n_0 n_1}{n^2}}
\end{equations}

where $M_1$ is the mean composite score for all correct responses; $M_0$ is the
mean composite score for incorrect responses; $n_0$, $n_1$, and $n$ are number
of incorrect, correct, and total responses, respectively; and the standard
deviation $s_n$ is defined as:

\begin{equations}
  s_n = \sqrt{ \frac{1}{n} \displaystyle\sum_{i=1}^n (X_i - \overline{X})^2} 
\end{equations}

where $X_i$ is the ith response and $\overline{X}$ is the mean of the
responses.  $\alpha$ values have range $[-1, 1]$.  Positive values indicate
that the question is positively discriminating; it is a question which is
useful for gauging the constructs which the composite score seeks to measure.
Students who succeed on a question with a higher $\alpha$ are more likely to
have higher composite scores.  Likewise, negative values indicate that the
question is inversely related to composite score.  Values near zero indicate
that the question is unrelated.

The probability of guessing is defined as

\begin{equations}
  \gamma_i = \frac{1}{m_i}
\end{equations}

where $m_i$ is the number of options in a multiple choice question.  It is
assumed that each option is equally likely to be chosen if a student with
very low trait ability were to answer the question; that is, there can be
no allowance for logical process-of-elimination tactics.

If the trait ability of the student is known in addition to the item
parameters, then the probability of a correct response can be calculated.
However, it is more often than not the case that the response set of the
student is known, and trait ability is unknown.  In such a case, a variety of
techniques are deployable.

Trait ability in IRT can be obtained using a maximum likelihood estimation
(MLE) method, which finds a maximum-likelihood estimate of $\theta$ by testing
a range of $\theta$ values with the IRT formula \cite{baker2004}.  Later it
will be shown how $\theta$ can be estimated.  Another potential technique is
Newton-Raphson, which in most other applications would be the more efficient
and desirable method.  It will later be discussed why the MLE method is
preferred.

%The utility of IRT is that it requires fewer questions to gauge trait ability
%due to its account of other factors ($\alpha, \beta, \gamma$). It is thus
%reputed to be a more mature means of assessment than CTT.

Until now, the fusion of IRT and Bloom's taxonomy has only existed in the
literature as a possibility \cite{sitthisak}.  This work seeks to reconcile
the two by offering a compatible interpretation of Bloom's taxonomy.
%TODO: (SRB) I don't think this is true. See email I sent you on 1/24/2017.

\subsection{Evaluation of Trait Ability}

Consider a set of content items or questions for which the answer is either
incorrect or correct.  This is true in the case of multiple choice questions
(as well as true-false, which is a subset of multiple-choice questions).

Suppose the student has a response set for content items: 

\begin{equations}
  \label{eq:responses}
  X_s = x_{s1}, x_{s2}, \ldots, x_{si}, \ldots x_{sn}
\end{equations}

Here $X_s$ is the response vector of student $s$, and $x_{si}$ is the
correctness of the response to content item $i$ by student $s$; it is zero if
the response is incorrect, or 1 if it is correct.  

These questions may be of various and sundry discriminations, difficulties, and
probabilities of guessing.  The first question may have $\alpha_1=.5$,
$\beta_1=1$, and $\gamma_1=.25$; the second may only differ in its difficulty,
for example $\beta_2=-1$.  However, it shall be assumed that the set of
content items for which the student has provided responses are for the same
concept and the same Bloom taxonomic level.  The reason for this assumption
will become apparent later.

Recall that the probability $p_{si}$ of student $s$ answering the $i^{th}$
question correctly is:

\begin{equations}
  p_{si}(\theta_s) = \gamma_i + \frac{1-\gamma_i}{1+e^{\alpha_i(\theta_s-\beta_i)}}
  \tag{\ref{eq:irt}}
\end{equations}

Since $\theta_s$ is unknown but the response set is known, one method for
determining $\theta_s$ is by guessing a range of possible values.  First, one
can define a function $f_{si}$:

\begin{equations}
f_{si}(\theta_s) =\left\{
         \begin{array}{ll}
               p_{si}(\theta_s) & \mathrm{if}\  x_{si} = 1 \\
               q_{si}(\theta_s) & \mathrm{otherwise}
         \end{array}
       \right.
\end{equations}
% TODO: (SRB) Do you mean p_{si} above when you type p_i?

where 

\begin{equations}
   q_{si}(\theta_s)  = 1 - p_{si}(\theta_s).
\end{equations}

That is, $f_{si}$ assumes the probability $p_{si}$ if answered correctly and
$q_{si}$ if not answered correctly.  Proceeding on the assumption that each of
the observations (that is, responses in the response set) is independent, the
probablity of observing a total response set given a particular $\theta_s$
value is the product of the probabilities $f_{si}$ for all $i$, $1 \leq i \leq
n$, or

\begin{equations}
  \prod_{i=1}^n f_{si}(\theta_s).
\end{equations}

Supposing that there exists some $\theta_s$ which maximizes this product,
the most likely value for the student's true trait ability $\theta_s$ is
defined by:

\begin{equations}
  \theta_s = 
  \underset{\theta}{\textrm{argmax}}
  \Bigg[ 
  \prod_{i=1}^n f_{si}(\theta).
  \Bigg]
\end{equations}


that is, that value of $\theta_s$ which maximizes the product which gives the
probability of all the observations occuring together, given $\theta$.  To
obtain this, products for a range of possible $\theta$ values are calculated.

In the intelligent tutoring system which has been constructed, there are only
thirteen such values, drawing from the +/- system.  The mapping of grades to
trait ability levels is given in Table~\ref{tab:grades}.  This mapping could
also be applied to difficulty levels of questions.  A ``C question'' is one
which students of average trait ability (and perhaps just more than half the
class) may be expected to answer; an ``A+ question'' is a very difficult
question which students of only A+ ability at the time of asking may be
expected to answer; and an ``F question'' is one which may be used to determine
if a student's trait ability is minimally satisfactory.  This mapping lends to
a intuitive understanding of trait ability--as the familiar letter grade.  An
MLE may thereby effectively grade the student.

% TODO: (SRB) Something should be said about the confidence in the trait
% ability assessment given above. In principle, a letter grade could be
% obtained from 2 questions.

To that end, rather than using a fine-grained MLE, it makes practical sense to
calculate products for these thirteen values of $\theta$, since higher
granularity than the +/- system is not useful for final grade assignment, nor
is necessarily more intuitive for the student or instructor.  Restricting the
products calculated to this set of values also makes sense from an efficiency
standpoint.  The total time complexity of the MLE is linear in the size of the
response set, regardless of the number of guesses in $\theta$; while not
asymptotically reduced from using a smaller step size, it is five times faster
than the typical choice of $10^{-1}$, and fifty times faster than the
fine-grained $10^{-2}$.  A graph calculating likelihoods for MLE is depicted in
Figure~\ref{fig:mle}.

% Talk about gradient descent and Newton-Raphson

The total number of MLE estimations for trait ability throughout a course is
equal to the total number of responses for all questions throughout a course,
which can be quite large; it is proportional to the number of students times
the number of items. 


\section{Previous Research}

% TODO

